{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b785a541-defb-4557-919d-0d6370790d62",
   "metadata": {},
   "source": [
    "## Yahoo Prices for Your `latest_ticker` Universe (with rate-limit, retry, and checkpoint)\n",
    "\n",
    "This block downloads **daily OHLCV** from Yahoo Finance (via `yfinance`) for every\n",
    "unique `latest_ticker` in your universe, with **dot↔hyphen** symbol fallback,\n",
    "simple **rate limiting**, **retries**, and **checkpointing** so a long run can be resumed.\n",
    "\n",
    "### What it pulls\n",
    "For each symbol (since `START_DATE`):\n",
    "- `date`, `open`, `high`, `low`, `close`, `adj_close`, `volume`\n",
    "- plus `ticker_latest` (the canonical ticker from your mapping step)\n",
    "\n",
    "### Key behaviors\n",
    "1. **Universe build**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6956ea38-cdaa-43ad-a2a9-050ec0a1e020",
   "metadata": {},
   "source": [
    "Ensures a clean, deduped list of symbols to fetch.\n",
    "\n",
    "2. **Yahoo symbol normalization**  \n",
    "- Primary: convert `.` to `-` for share classes (e.g., `BRK.B` → `BRK-B`).\n",
    "- Fallback: if no data, try the **opposite** variant once (`-` ↔ `.`).\n",
    "\n",
    "3. **Per-symbol fetch with retry**  \n",
    "- `yf.Ticker(symbol).history(...)` with `auto_adjust=False`, `actions=False`.\n",
    "- Up to `MAX_RETRY` attempts per symbol (exponential backoff).\n",
    "\n",
    "4. **Rate limiting**  \n",
    "- A simple rolling window cap (`CALLS_PER_MIN`) to avoid throttling.\n",
    "- Pauses when the minute budget is reached; then resumes.\n",
    "\n",
    "5. **Checkpointing (Parquet)**  \n",
    "- Every `CHECKPOINT_EVERY` symbols, appends progress to\n",
    "  `prices_checkpoint.parquet`.  \n",
    "- On restart, previously completed tickers are **skipped** automatically.\n",
    "\n",
    "6. **Tidy + dedupe**  \n",
    "- Resets index to a flat table, renames columns, enforces datetime→date,\n",
    "  sorts by (`ticker_latest`, `date`), and drops any duplicate day rows.\n",
    "\n",
    "### Output\n",
    "- **`prices_df`** with columns:\n",
    "- `date` (Python `date`), `open`, `high`, `low`, `close`, `adj_close`, `volume`, `ticker_latest`\n",
    "\n",
    "This is ready to:\n",
    "- join to your S&P membership (on `ticker_latest` + `date` within membership ranges),  \n",
    "- compute returns, rolling stats, risk metrics, etc.,  \n",
    "- or bulk-load to PostgreSQL with a primary key on (`ticker_latest`, `date`).\n",
    "\n",
    "### Usage\n",
    "- Set `START_DATE` to match your analysis window.\n",
    "- Build the `universe` from your enriched table (`df[\"latest_ticker\"]`).\n",
    "- Run:\n",
    "```python\n",
    "prices_df = fetch_prices_for_universe(\n",
    " universe,\n",
    " checkpoint_path=\"prices_checkpoint.parquet\"  # optional but recommended\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85627d34-e34e-444f-b922-3280cb19fb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Universe size: 679\n"
     ]
    }
   ],
   "source": [
    "# !pip install sqlalchemy psycopg2-binary\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Prefer env vars; fall back to your local creds\n",
    "pg_user = os.getenv(\"PGUSER\", \"postgres\")\n",
    "pg_pass = os.getenv(\"PGPASSWORD\", \"CSDBMS623\")\n",
    "pg_host = os.getenv(\"PGHOST\", \"localhost\")\n",
    "pg_port = os.getenv(\"PGPORT\", \"5432\")\n",
    "pg_db   = os.getenv(\"PGDATABASE\", \"SP500_ML\")\n",
    "\n",
    "engine = create_engine(f\"postgresql+psycopg2://{pg_user}:{pg_pass}@{pg_host}:{pg_port}/{pg_db}\")\n",
    "\n",
    "# 1) If you want the full enriched daily table:\n",
    "profiles_df = pd.read_sql_query(\"\"\"\n",
    "    SELECT\n",
    "        date,\n",
    "        ticker           AS ticker_membership,\n",
    "        latest_ticker,\n",
    "        company_name,\n",
    "        sector,\n",
    "        currency,\n",
    "        is_actively_trading,\n",
    "        cik\n",
    "    FROM sp500_long_latest_profiles\n",
    "\"\"\", engine, parse_dates=[\"date\"])\n",
    "\n",
    "# 2) If you just need the symbol universe for prices (MUCH faster/lighter):\n",
    "universe_df = pd.read_sql_query(\"\"\"\n",
    "    SELECT DISTINCT latest_ticker\n",
    "    FROM sp500_long_latest_profiles\n",
    "    WHERE latest_ticker IS NOT NULL\n",
    "\"\"\", engine)\n",
    "\n",
    "# (Optional) restrict to the most recent date only:\n",
    "# universe_df = pd.read_sql_query(\"\"\"\n",
    "#     SELECT DISTINCT latest_ticker\n",
    "#     FROM sp500_long_latest_profiles\n",
    "#     WHERE latest_ticker IS NOT NULL\n",
    "#       AND date = (SELECT MAX(date) FROM sp500_long_latest_profiles)\n",
    "# \"\"\", engine)\n",
    "\n",
    "universe = (universe_df[\"latest_ticker\"]\n",
    "            .dropna().map(lambda s: str(s).strip().upper())\n",
    "            .drop_duplicates().tolist())\n",
    "\n",
    "print(\"Universe size:\", len(universe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa113ec7-25c2-4514-808b-0ae40649d125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Universe size: 679\n"
     ]
    }
   ],
   "source": [
    "# !pip install sqlalchemy psycopg2-binary\n",
    "import os, pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "pg_user = os.getenv(\"PGUSER\", \"postgres\")\n",
    "pg_pass = os.getenv(\"PGPASSWORD\", \"CSDBMS623\")\n",
    "pg_host = os.getenv(\"PGHOST\", \"localhost\")\n",
    "pg_port = os.getenv(\"PGPORT\", \"5432\")\n",
    "pg_db   = os.getenv(\"PGDATABASE\", \"SP500_ML\")\n",
    "\n",
    "engine = create_engine(f\"postgresql+psycopg2://{pg_user}:{pg_pass}@{pg_host}:{pg_port}/{pg_db}\")\n",
    "\n",
    "# Use only the most recent membership date to define the current universe\n",
    "universe = pd.read_sql_query(\"\"\"\n",
    "    SELECT DISTINCT UPPER(TRIM(latest_ticker)) AS latest_ticker\n",
    "    FROM sp500_long_latest_profiles\n",
    "    WHERE latest_ticker IS NOT NULL\n",
    "\n",
    "\"\"\", engine)[\"latest_ticker\"].tolist()\n",
    "\n",
    "print(\"Universe size:\", len(universe))\n",
    "\n",
    "# Now run your Yahoo fetcher\n",
    "#prices_df = fetch_prices_for_universe(universe, checkpoint_path=\"prices_checkpoint.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fca69c8-aef4-4a44-9768-04e8405a1803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming: 601 symbols already saved at prices_checkpoint.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$ABMD: possibly delisted; no timezone found\n",
      "$ALTR: possibly delisted; no timezone found\n",
      "$ALXN: possibly delisted; no timezone found\n",
      "$ARG: possibly delisted; no price data found  (1d 2013-09-07 -> 2025-09-27)\n",
      "$ATVI: possibly delisted; no timezone found\n",
      "$BRCM: possibly delisted; no price data found  (1d 2013-09-07 -> 2025-09-27)\n",
      "$CAM: possibly delisted; no price data found  (1d 2013-09-07 -> 2025-09-27)\n",
      "$CELG: possibly delisted; no timezone found\n",
      "$CERN: possibly delisted; no timezone found\n",
      "$CPGX: possibly delisted; no price data found  (1d 2013-09-07 -> 2025-09-27)\n",
      "$CTLT: possibly delisted; no timezone found\n",
      "$CTXS: possibly delisted; no timezone found\n",
      "$DFS: possibly delisted; no timezone found\n",
      "$DISH: possibly delisted; no timezone found\n",
      "$DO: possibly delisted; no timezone found\n",
      "$DRE: possibly delisted; no timezone found\n",
      "$ENDP: possibly delisted; no timezone found\n",
      "$FRC: possibly delisted; no timezone found\n",
      "$GMCR: possibly delisted; no price data found  (1d 2013-09-07 -> 2025-09-27)\n",
      "$HFC: possibly delisted; no timezone found\n",
      "$JOY: possibly delisted; no price data found  (1d 2013-09-07 -> 2025-09-27)\n",
      "$JWN: possibly delisted; no timezone found\n",
      "$KSU: possibly delisted; no timezone found\n",
      "$LLTC: possibly delisted; no price data found  (1d 2013-09-07 -> 2025-09-27)\n",
      "$LSI: possibly delisted; no timezone found\n",
      "$LVLT: possibly delisted; no price data found  (1d 2013-09-07 -> 2025-09-27)\n",
      "$MJN: possibly delisted; no price data found  (1d 2013-09-07 -> 2025-09-27)\n",
      "$MNK: possibly delisted; no timezone found\n",
      "$MON: possibly delisted; no timezone found\n",
      "$MRO: possibly delisted; no timezone found\n",
      "$MXIM: possibly delisted; no timezone found\n",
      "$NLSN: possibly delisted; no timezone found\n",
      "$NYX: possibly delisted; no price data found  (1d 2013-09-07 -> 2025-09-27)\n",
      "$PBCT: possibly delisted; no timezone found\n",
      "$PDCO: possibly delisted; no timezone found\n",
      "$POM: possibly delisted; no price data found  (1d 2013-09-07 -> 2025-09-27)\n",
      "$PXD: possibly delisted; no timezone found\n",
      "$RAI: possibly delisted; no price data found  (1d 2013-09-07 -> 2025-09-27)\n",
      "$RMO: possibly delisted; no timezone found\n",
      "$SIVB: possibly delisted; no timezone found\n",
      "$SPLS: possibly delisted; no price data found  (1d 2013-09-07 -> 2025-09-27)\n",
      "$SRCL: possibly delisted; no timezone found\n",
      "$STJ: possibly delisted; no price data found  (1d 2013-09-07 -> 2025-09-27)\n",
      "$SWN: possibly delisted; no timezone found\n",
      "$TWTR: possibly delisted; no timezone found\n",
      "$WFM: possibly delisted; no price data found  (1d 2013-09-07 -> 2025-09-27)\n",
      "$WRK: possibly delisted; no timezone found\n",
      "$X: possibly delisted; no timezone found\n",
      "$XEC: possibly delisted; no timezone found\n",
      "$XLNX: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prices rows: 1787822\n",
      "         date       open       high        low      close  adj_close   volume  \\\n",
      "0  2013-09-03  33.648067  33.984264  33.347637  33.569386  30.354403  2188709   \n",
      "1  2013-09-04  33.569386  34.306152  33.433475  34.241776  30.962395  3525756   \n",
      "2  2013-09-05  34.206009  34.399143  34.034336  34.105865  30.839495  1835854   \n",
      "3  2013-09-06  34.127323  34.184547  33.712444  33.927040  30.677790  1863394   \n",
      "4  2013-09-09  34.041489  34.370529  33.826897  34.299000  31.014139  2333122   \n",
      "\n",
      "  ticker_latest  \n",
      "0             A  \n",
      "1             A  \n",
      "2             A  \n",
      "3             A  \n",
      "4             A  \n"
     ]
    }
   ],
   "source": [
    "# ================== Yahoo prices for your latest_ticker universe (rate-limited) ==================\n",
    "# pip install yfinance\n",
    "import time, math, traceback\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "START_DATE = \"2013-09-07\"   # same start you used for membership\n",
    "MAX_RETRY  = 3\n",
    "CALLS_PER_MIN = 290         # under Yahoo-ish caps\n",
    "CHECKPOINT_EVERY = 100      # persist every N symbols to avoid rework\n",
    "\n",
    "def to_yahoo(sym: str) -> str:\n",
    "    s = str(sym).strip().upper()\n",
    "    # Yahoo prefers hyphens for share classes (BRK-B, BF-B, etc.)\n",
    "    return s.replace(\".\", \"-\")\n",
    "\n",
    "def swap_variant(s: str) -> str:\n",
    "    # Try the other style if the first call returns empty\n",
    "    return s.replace(\"-\", \".\") if \"-\" in s else s.replace(\".\", \"-\")\n",
    "\n",
    "def fetch_one(symbol: str):\n",
    "    \"\"\"\n",
    "    Fetch one ticker (with dot/hyphen fallback).\n",
    "    Returns a tidy DataFrame: date, open, high, low, close, adj_close, volume, ticker_latest\n",
    "    or None if nothing available.\n",
    "    \"\"\"\n",
    "    def _history(tkr):\n",
    "        t = yf.Ticker(tkr)\n",
    "        return t.history(start=START_DATE, end=None, interval=\"1d\", auto_adjust=False, actions=False)\n",
    "\n",
    "    # try primary\n",
    "    hist = _history(symbol)\n",
    "    if hist is None or hist.empty:\n",
    "        # try variant once\n",
    "        alt = swap_variant(symbol)\n",
    "        if alt != symbol:\n",
    "            hist = _history(alt)\n",
    "\n",
    "    if hist is None or hist.empty:\n",
    "        return None\n",
    "\n",
    "    out = (\n",
    "        hist.reset_index()\n",
    "            .rename(columns={\n",
    "                \"Date\":\"date\", \"Open\":\"open\", \"High\":\"high\", \"Low\":\"low\",\n",
    "                \"Close\":\"close\", \"Adj Close\":\"adj_close\", \"Volume\":\"volume\"\n",
    "            })\n",
    "            [[\"date\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\"]]\n",
    "    )\n",
    "    return out\n",
    "\n",
    "def fetch_prices_for_universe(universe, checkpoint_path=None):\n",
    "    \"\"\"\n",
    "    universe: iterable of your canonical symbols (e.g., df['latest_ticker'].unique()).\n",
    "    checkpoint_path: optional parquet path to append progress.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    calls_in_window = 0\n",
    "    window_start = time.time()\n",
    "    done = 0\n",
    "\n",
    "    # If resuming from checkpoint, skip already completed tickers\n",
    "    done_set = set()\n",
    "    if checkpoint_path:\n",
    "        try:\n",
    "            prev = pd.read_parquet(checkpoint_path, columns=[\"ticker_latest\"])\n",
    "            done_set = set(prev[\"ticker_latest\"].unique())\n",
    "            print(f\"Resuming: {len(done_set)} symbols already saved at {checkpoint_path}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    for i, sym in enumerate(universe, start=1):\n",
    "        if sym in done_set:\n",
    "            continue\n",
    "\n",
    "        ysym = to_yahoo(sym)\n",
    "        # --- Rate limit: <= CALLS_PER_MIN per rolling minute\n",
    "        now = time.time()\n",
    "        elapsed = now - window_start\n",
    "        if calls_in_window >= CALLS_PER_MIN and elapsed < 60:\n",
    "            sleep_for = 60 - elapsed\n",
    "            print(f\"Pausing {sleep_for:.1f}s to respect {CALLS_PER_MIN}/min (done {i-1}/{len(universe)})\")\n",
    "            time.sleep(sleep_for)\n",
    "            window_start = time.time()\n",
    "            calls_in_window = 0\n",
    "\n",
    "        payload = None\n",
    "        backoff = 2.0\n",
    "        for attempt in range(1, MAX_RETRY + 1):\n",
    "            try:\n",
    "                payload = fetch_one(ysym)\n",
    "                calls_in_window += 1    # count the request (history call)\n",
    "                break\n",
    "            except Exception:\n",
    "                if attempt < MAX_RETRY:\n",
    "                    time.sleep(backoff); backoff *= 2\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"[WARN] {sym}: failed after retries\\n{traceback.format_exc(limit=1)}\")\n",
    "\n",
    "        if payload is None or payload.empty:\n",
    "            # nothing available; still record a stub?\n",
    "            pass\n",
    "        else:\n",
    "            payload[\"ticker_latest\"] = sym\n",
    "            rows.append(payload)\n",
    "\n",
    "        done += 1\n",
    "\n",
    "        # checkpoint every N tickers\n",
    "        if checkpoint_path and (done % CHECKPOINT_EVERY == 0):\n",
    "            ck = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(\n",
    "                columns=[\"date\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\",\"ticker_latest\"]\n",
    "            )\n",
    "            if not ck.empty:\n",
    "                # append or write new\n",
    "                try:\n",
    "                    prev = pd.read_parquet(checkpoint_path)\n",
    "                    ck = pd.concat([prev, ck], ignore_index=True)\n",
    "                except Exception:\n",
    "                    pass\n",
    "                ck.to_parquet(checkpoint_path, index=False)\n",
    "                rows = []  # clear buffer\n",
    "                print(f\"Checkpointed {done} symbols → {checkpoint_path}\")\n",
    "\n",
    "    # final concat\n",
    "    prices = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(\n",
    "        columns=[\"date\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\",\"ticker_latest\"]\n",
    "    )\n",
    "    # if we had checkpointing, merge the final buffer with file\n",
    "    if checkpoint_path:\n",
    "        try:\n",
    "            prev = pd.read_parquet(checkpoint_path)\n",
    "            prices = pd.concat([prev, prices], ignore_index=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # tidy types\n",
    "    if not prices.empty:\n",
    "        prices[\"date\"] = pd.to_datetime(prices[\"date\"], errors=\"coerce\").dt.date\n",
    "        prices = (\n",
    "            prices.sort_values([\"ticker_latest\",\"date\"])\n",
    "                  .drop_duplicates(subset=[\"ticker_latest\",\"date\"], keep=\"last\")\n",
    "                  .reset_index(drop=True)\n",
    "        )\n",
    "    return prices\n",
    "\n",
    "# ---------- Build your universe from your table `df` ----------\n",
    "#universe = (\n",
    "    #pd.Series(df[\"latest_ticker\"])\n",
    "      #.dropna().map(lambda s: str(s).strip().upper())\n",
    "      #.drop_duplicates().tolist()\n",
    "#)\n",
    "#print(\"Unique latest_ticker count:\", len(universe))\n",
    "\n",
    "# ---------- Run (with checkpoint to survive interruptions) ----------\n",
    "prices_df = fetch_prices_for_universe(universe, checkpoint_path=\"prices_checkpoint.parquet\")\n",
    "print(\"Prices rows:\", len(prices_df))\n",
    "print(prices_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa43df66-cdee-41a1-ab87-2a5efaac3bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ticker_latest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-09-03</td>\n",
       "      <td>33.648067</td>\n",
       "      <td>33.984264</td>\n",
       "      <td>33.347637</td>\n",
       "      <td>33.569386</td>\n",
       "      <td>30.354403</td>\n",
       "      <td>2188709</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-09-04</td>\n",
       "      <td>33.569386</td>\n",
       "      <td>34.306152</td>\n",
       "      <td>33.433475</td>\n",
       "      <td>34.241776</td>\n",
       "      <td>30.962395</td>\n",
       "      <td>3525756</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-09-05</td>\n",
       "      <td>34.206009</td>\n",
       "      <td>34.399143</td>\n",
       "      <td>34.034336</td>\n",
       "      <td>34.105865</td>\n",
       "      <td>30.839495</td>\n",
       "      <td>1835854</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-09-06</td>\n",
       "      <td>34.127323</td>\n",
       "      <td>34.184547</td>\n",
       "      <td>33.712444</td>\n",
       "      <td>33.927040</td>\n",
       "      <td>30.677790</td>\n",
       "      <td>1863394</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-09-09</td>\n",
       "      <td>34.041489</td>\n",
       "      <td>34.370529</td>\n",
       "      <td>33.826897</td>\n",
       "      <td>34.299000</td>\n",
       "      <td>31.014139</td>\n",
       "      <td>2333122</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1787817</th>\n",
       "      <td>2025-08-25</td>\n",
       "      <td>156.759995</td>\n",
       "      <td>157.210007</td>\n",
       "      <td>154.889999</td>\n",
       "      <td>155.190002</td>\n",
       "      <td>155.190002</td>\n",
       "      <td>1797700</td>\n",
       "      <td>ZTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1787818</th>\n",
       "      <td>2025-08-26</td>\n",
       "      <td>155.380005</td>\n",
       "      <td>156.320007</td>\n",
       "      <td>154.509995</td>\n",
       "      <td>154.789993</td>\n",
       "      <td>154.789993</td>\n",
       "      <td>3614300</td>\n",
       "      <td>ZTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1787819</th>\n",
       "      <td>2025-08-27</td>\n",
       "      <td>155.160004</td>\n",
       "      <td>156.110001</td>\n",
       "      <td>154.509995</td>\n",
       "      <td>155.369995</td>\n",
       "      <td>155.369995</td>\n",
       "      <td>1931100</td>\n",
       "      <td>ZTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1787820</th>\n",
       "      <td>2025-08-28</td>\n",
       "      <td>155.139999</td>\n",
       "      <td>155.350006</td>\n",
       "      <td>153.289993</td>\n",
       "      <td>154.789993</td>\n",
       "      <td>154.789993</td>\n",
       "      <td>1831500</td>\n",
       "      <td>ZTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1787821</th>\n",
       "      <td>2025-08-29</td>\n",
       "      <td>154.639999</td>\n",
       "      <td>156.490005</td>\n",
       "      <td>154.070007</td>\n",
       "      <td>156.399994</td>\n",
       "      <td>156.399994</td>\n",
       "      <td>1534600</td>\n",
       "      <td>ZTS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1787822 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               date        open        high         low       close  \\\n",
       "0        2013-09-03   33.648067   33.984264   33.347637   33.569386   \n",
       "1        2013-09-04   33.569386   34.306152   33.433475   34.241776   \n",
       "2        2013-09-05   34.206009   34.399143   34.034336   34.105865   \n",
       "3        2013-09-06   34.127323   34.184547   33.712444   33.927040   \n",
       "4        2013-09-09   34.041489   34.370529   33.826897   34.299000   \n",
       "...             ...         ...         ...         ...         ...   \n",
       "1787817  2025-08-25  156.759995  157.210007  154.889999  155.190002   \n",
       "1787818  2025-08-26  155.380005  156.320007  154.509995  154.789993   \n",
       "1787819  2025-08-27  155.160004  156.110001  154.509995  155.369995   \n",
       "1787820  2025-08-28  155.139999  155.350006  153.289993  154.789993   \n",
       "1787821  2025-08-29  154.639999  156.490005  154.070007  156.399994   \n",
       "\n",
       "          adj_close   volume ticker_latest  \n",
       "0         30.354403  2188709             A  \n",
       "1         30.962395  3525756             A  \n",
       "2         30.839495  1835854             A  \n",
       "3         30.677790  1863394             A  \n",
       "4         31.014139  2333122             A  \n",
       "...             ...      ...           ...  \n",
       "1787817  155.190002  1797700           ZTS  \n",
       "1787818  154.789993  3614300           ZTS  \n",
       "1787819  155.369995  1931100           ZTS  \n",
       "1787820  154.789993  1831500           ZTS  \n",
       "1787821  156.399994  1534600           ZTS  \n",
       "\n",
       "[1787822 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc27ae7-3ebb-4776-b0d9-3ed84cfbbbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"postgresql://postgres:CSDBMS623@localhost:5432/SP500_ML\")\n",
    "\n",
    "# 1) ensure target table exists (idempotent)\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS \"{TABLE}\" (\n",
    "            date date NOT NULL,\n",
    "            ticker_latest varchar(16) NOT NULL,\n",
    "            open double precision,\n",
    "            high double precision,\n",
    "            low double precision,\n",
    "            close double precision,\n",
    "            adj_close double precision,\n",
    "            volume bigint,\n",
    "            PRIMARY KEY (date, ticker_latest)\n",
    "        );\n",
    "    \"\"\"))\n",
    "\n",
    "# 2) stage the new data\n",
    "prices_clean.to_sql(\n",
    "    STAGING, engine, if_exists=\"replace\", index=False, method=\"multi\", chunksize=50_000,\n",
    "    dtype={\n",
    "        \"date\": DATE(),\n",
    "        \"ticker_latest\": VARCHAR(16),\n",
    "        \"open\": DOUBLE_PRECISION(),\n",
    "        \"high\": DOUBLE_PRECISION(),\n",
    "        \"low\": DOUBLE_PRECISION(),\n",
    "        \"close\": DOUBLE_PRECISION(),\n",
    "        \"adj_close\": DOUBLE_PRECISION(),\n",
    "        \"volume\": BIGINT(),\n",
    "    }\n",
    ")\n",
    "\n",
    "# 3) merge into target with upsert, then drop staging\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(f\"\"\"\n",
    "        INSERT INTO \"{TABLE}\" (date, ticker_latest, open, high, low, close, adj_close, volume)\n",
    "        SELECT s.date, s.ticker_latest, s.open, s.high, s.low, s.close, s.adj_close, s.volume\n",
    "        FROM \"{STAGING}\" s\n",
    "        ON CONFLICT (date, ticker_latest) DO UPDATE\n",
    "        SET open = EXCLUDED.open,\n",
    "            high = EXCLUDED.high,\n",
    "            low = EXCLUDED.low,\n",
    "            close = EXCLUDED.close,\n",
    "            adj_close = EXCLUDED.adj_close,\n",
    "            volume = EXCLUDED.volume;\n",
    "        DROP TABLE \"{STAGING}\";\n",
    "    \"\"\"))\n",
    "\n",
    "print(\"Upsert complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d563608e-9214-4a90-abb0-70281268859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# 0) Make sure your stock df is sorted and has proper dtypes\n",
    "df = prices_df.copy()\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df = df.sort_values([\"ticker_latest\",\"date\"])\n",
    "df[\"ret\"] = df.groupby(\"ticker_latest\", group_keys=False)[\"adj_close\"].pct_change()\n",
    "\n",
    "# 1) Download benchmark (^GSPC or 'SPY')\n",
    "start_date = df[\"date\"].min().normalize()\n",
    "mkt = yf.download(\"^GSPC\", start=start_date, auto_adjust=False, progress=False)\n",
    "\n",
    "# 2) FLATTEN possible MultiIndex columns from yfinance\n",
    "if isinstance(mkt.columns, pd.MultiIndex):\n",
    "    mkt.columns = [\"_\".join([str(x) for x in tup if x not in (None, \"\")]) for tup in mkt.columns]\n",
    "# Standardize expected column name for adjusted close\n",
    "if \"Adj Close\" in mkt.columns:\n",
    "    mkt = mkt.rename(columns={\"Adj Close\": \"adj_close_mkt\"})\n",
    "elif \"Adj Close_^GSPC\" in mkt.columns:\n",
    "    mkt = mkt.rename(columns={\"Adj Close_^GSPC\": \"adj_close_mkt\"})\n",
    "elif \"Adj_Close\" in mkt.columns:\n",
    "    mkt = mkt.rename(columns={\"Adj_Close\": \"adj_close_mkt\"})\n",
    "else:\n",
    "    # last resort: take the first column named like adj close\n",
    "    cand = [c for c in mkt.columns if \"adj\" in c.lower() and \"close\" in c.lower()]\n",
    "    assert cand, \"Couldn't find adjusted close in market DataFrame\"\n",
    "    mkt = mkt.rename(columns={cand[0]: \"adj_close_mkt\"})\n",
    "\n",
    "# 3) Make date a normal column and compute market returns\n",
    "mkt = mkt.rename_axis(\"date\").reset_index()\n",
    "mkt[\"date\"] = pd.to_datetime(mkt[\"date\"])\n",
    "mkt[\"mkt_ret\"] = mkt[\"adj_close_mkt\"].pct_change()\n",
    "\n",
    "# 4) Now the merge will work (both have 1-level columns and datetime 'date')\n",
    "x = df.merge(mkt[[\"date\",\"mkt_ret\"]], on=\"date\", how=\"left\", validate=\"m:1\")\n",
    "\n",
    "# 5) Rolling 12-month beta (~252 trading days; require at least 126 obs)\n",
    "W = 252\n",
    "MIN_OBS = 126\n",
    "def rolling_beta(g):\n",
    "    exy = (g[\"ret\"] * g[\"mkt_ret\"]).rolling(W, min_periods=MIN_OBS).mean()\n",
    "    ex  = g[\"ret\"].rolling(W, min_periods=MIN_OBS).mean()\n",
    "    ey  = g[\"mkt_ret\"].rolling(W, min_periods=MIN_OBS).mean()\n",
    "    cov = exy - ex*ey\n",
    "    var = g[\"mkt_ret\"].rolling(W, min_periods=MIN_OBS).var()\n",
    "    return cov / var\n",
    "\n",
    "x[\"beta_12m\"] = x.groupby(\"ticker_latest\", group_keys=False).apply(rolling_beta)\n",
    "beta_df = x.loc[:, [\"date\",\"ticker_latest\",\"beta_12m\"]].dropna().reset_index(drop=True)\n",
    "print(beta_df.head(), \"\\nrows:\", len(beta_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c3094f-1b70-4431-9fd3-3bc2f34b917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "beta_clean = beta_df.copy()\n",
    "beta_clean[\"date\"] = pd.to_datetime(beta_clean[\"date\"], errors=\"coerce\").dt.date\n",
    "beta_clean[\"ticker_latest\"] = (\n",
    "    beta_clean[\"ticker_latest\"].astype(str).str.strip().str.upper()\n",
    ")\n",
    "beta_clean = (\n",
    "    beta_clean.dropna(subset=[\"date\", \"ticker_latest\", \"beta_12m\"])\n",
    "              .drop_duplicates(subset=[\"date\",\"ticker_latest\"], keep=\"last\")\n",
    "              .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"rows to ingest:\", len(beta_clean))\n",
    "beta_clean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ca23d1-d1ae-4054-85cf-1b515f05a633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Upsert into Postgres ---\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.dialects.postgresql import DATE, VARCHAR, DOUBLE_PRECISION\n",
    "\n",
    "ENGINE_URL = \"postgresql://postgres:CSDBMS623@localhost:5432/SP500_ML\"\n",
    "TABLE      = \"sp500_beta_12m_daily\"\n",
    "STAGING    = TABLE + \"_stg\"\n",
    "\n",
    "engine = create_engine(ENGINE_URL)\n",
    "\n",
    "# 1) Ensure target table exists with PK (idempotent)\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS \"{TABLE}\" (\n",
    "            date date NOT NULL,\n",
    "            ticker_latest varchar(16) NOT NULL,\n",
    "            beta_12m double precision,\n",
    "            PRIMARY KEY (date, ticker_latest)\n",
    "        );\n",
    "    \"\"\"))\n",
    "\n",
    "# 2) Stage new data\n",
    "beta_clean.to_sql(\n",
    "    STAGING, engine, if_exists=\"replace\", index=False, method=\"multi\", chunksize=50_000,\n",
    "    dtype={\n",
    "        \"date\": DATE(),\n",
    "        \"ticker_latest\": VARCHAR(16),\n",
    "        \"beta_12m\": DOUBLE_PRECISION(),\n",
    "    },\n",
    ")\n",
    "\n",
    "# 3) Merge (UPSERT) from staging into target, then drop staging\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(f\"\"\"\n",
    "        INSERT INTO \"{TABLE}\" (date, ticker_latest, beta_12m)\n",
    "        SELECT s.date, s.ticker_latest, s.beta_12m\n",
    "        FROM \"{STAGING}\" s\n",
    "        ON CONFLICT (date, ticker_latest) DO UPDATE\n",
    "            SET beta_12m = EXCLUDED.beta_12m;\n",
    "        DROP TABLE \"{STAGING}\";\n",
    "    \"\"\"))\n",
    "\n",
    "print(\"Upsert complete →\", TABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b0b04b5c-775d-474f-ac5a-48d79dfaa145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sqlalchemy psycopg2-binary\n",
    "import os, pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "pg_user = os.getenv(\"PGUSER\", \"postgres\")\n",
    "pg_pass = os.getenv(\"PGPASSWORD\", \"CSDBMS623\")\n",
    "pg_host = os.getenv(\"PGHOST\", \"localhost\")\n",
    "pg_port = os.getenv(\"PGPORT\", \"5432\")\n",
    "pg_db   = os.getenv(\"PGDATABASE\", \"SP500_ML\")\n",
    "\n",
    "engine = create_engine(f\"postgresql+psycopg2://{pg_user}:{pg_pass}@{pg_host}:{pg_port}/{pg_db}\")\n",
    "\n",
    "# Use only the most recent membership date to define the current universe\n",
    "df = pd.read_sql_query(\"\"\"\n",
    "    SELECT * FROM sp500_prices_daily_yahoo\n",
    "\n",
    "\"\"\", engine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "264e6857-637f-4f09-b760-aa6c140c54b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker_latest</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>volume</th>\n",
       "      <th>company_sk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-09-03</td>\n",
       "      <td>A</td>\n",
       "      <td>33.648067</td>\n",
       "      <td>33.984264</td>\n",
       "      <td>33.347637</td>\n",
       "      <td>33.569386</td>\n",
       "      <td>30.354403</td>\n",
       "      <td>2188709</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-09-04</td>\n",
       "      <td>A</td>\n",
       "      <td>33.569386</td>\n",
       "      <td>34.306152</td>\n",
       "      <td>33.433475</td>\n",
       "      <td>34.241776</td>\n",
       "      <td>30.962395</td>\n",
       "      <td>3525756</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-09-05</td>\n",
       "      <td>A</td>\n",
       "      <td>34.206009</td>\n",
       "      <td>34.399143</td>\n",
       "      <td>34.034336</td>\n",
       "      <td>34.105865</td>\n",
       "      <td>30.839495</td>\n",
       "      <td>1835854</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-09-06</td>\n",
       "      <td>A</td>\n",
       "      <td>34.127323</td>\n",
       "      <td>34.184547</td>\n",
       "      <td>33.712444</td>\n",
       "      <td>33.927040</td>\n",
       "      <td>30.677790</td>\n",
       "      <td>1863394</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-09-09</td>\n",
       "      <td>A</td>\n",
       "      <td>34.041489</td>\n",
       "      <td>34.370529</td>\n",
       "      <td>33.826897</td>\n",
       "      <td>34.299000</td>\n",
       "      <td>31.014126</td>\n",
       "      <td>2333122</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797760</th>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>146.550003</td>\n",
       "      <td>146.710007</td>\n",
       "      <td>144.350006</td>\n",
       "      <td>144.649994</td>\n",
       "      <td>144.649994</td>\n",
       "      <td>2044700</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797761</th>\n",
       "      <td>2025-09-23</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>143.250000</td>\n",
       "      <td>146.179993</td>\n",
       "      <td>141.529999</td>\n",
       "      <td>142.610001</td>\n",
       "      <td>142.610001</td>\n",
       "      <td>3679500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797762</th>\n",
       "      <td>2025-09-24</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>141.619995</td>\n",
       "      <td>144.009995</td>\n",
       "      <td>140.539993</td>\n",
       "      <td>141.669998</td>\n",
       "      <td>141.669998</td>\n",
       "      <td>4224500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797763</th>\n",
       "      <td>2025-09-25</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>141.330002</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>139.339996</td>\n",
       "      <td>141.130005</td>\n",
       "      <td>141.130005</td>\n",
       "      <td>3058500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797764</th>\n",
       "      <td>2025-09-26</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>141.860001</td>\n",
       "      <td>143.789993</td>\n",
       "      <td>141.270004</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>2575200</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1797765 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               date ticker_latest        open        high         low  \\\n",
       "0        2013-09-03             A   33.648067   33.984264   33.347637   \n",
       "1        2013-09-04             A   33.569386   34.306152   33.433475   \n",
       "2        2013-09-05             A   34.206009   34.399143   34.034336   \n",
       "3        2013-09-06             A   34.127323   34.184547   33.712444   \n",
       "4        2013-09-09             A   34.041489   34.370529   33.826897   \n",
       "...             ...           ...         ...         ...         ...   \n",
       "1797760  2025-09-22           ZTS  146.550003  146.710007  144.350006   \n",
       "1797761  2025-09-23           ZTS  143.250000  146.179993  141.529999   \n",
       "1797762  2025-09-24           ZTS  141.619995  144.009995  140.539993   \n",
       "1797763  2025-09-25           ZTS  141.330002  142.000000  139.339996   \n",
       "1797764  2025-09-26           ZTS  141.860001  143.789993  141.270004   \n",
       "\n",
       "              close   adj_close   volume  company_sk  \n",
       "0         33.569386   30.354403  2188709         NaN  \n",
       "1         34.241776   30.962395  3525756         NaN  \n",
       "2         34.105865   30.839495  1835854         NaN  \n",
       "3         33.927040   30.677790  1863394         NaN  \n",
       "4         34.299000   31.014126  2333122         NaN  \n",
       "...             ...         ...      ...         ...  \n",
       "1797760  144.649994  144.649994  2044700         NaN  \n",
       "1797761  142.610001  142.610001  3679500         NaN  \n",
       "1797762  141.669998  141.669998  4224500         NaN  \n",
       "1797763  141.130005  141.130005  3058500         NaN  \n",
       "1797764  143.500000  143.500000  2575200         NaN  \n",
       "\n",
       "[1797765 rows x 9 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0300df5f-43f3-44a9-8182-1ab4897e48de",
   "metadata": {},
   "source": [
    "## Technical Momentum & TA Features (per-ticker)\n",
    "\n",
    "This cell engineers **momentum** and **technical indicators** for each `ticker_latest`\n",
    "using `pandas_ta`. It assumes `df` has at least `['ticker_latest', 'date', 'adj_close']`,\n",
    "is **sorted by (`ticker_latest`, `date`)**, and `date` is a proper `datetime`.\n",
    "\n",
    "### Features created\n",
    "- **Momentum (close-to-close returns):**\n",
    "  - `30_day_return`  ≈ 1-month ~21 trading days → `pct_change(periods=21)`\n",
    "  - `180_day_return` ≈ 6-months ~126 trading days → `pct_change(periods=126)`\n",
    "  - `360_day_return` ≈ 12-months ~252 trading days → `pct_change(periods=252)`\n",
    "  - Each is computed **per ticker** via `groupby('ticker_latest').transform(...)`.\n",
    "\n",
    "- **RSI (Relative Strength Index):**\n",
    "  - `rsi` (14), `rsi2` (9), `rsi3` (3) using `pandas_ta.rsi`.\n",
    "  - Shorter lengths react faster but are noisier.\n",
    "\n",
    "- **Simple Moving Averages (SMA):**\n",
    "  - `sma` (50), `sma2` (100), `sma3` (200) via `pandas_ta.sma`.\n",
    "\n",
    "- **Bollinger Bands (20, 2σ):**\n",
    "  - Grouped `bbands = pandas_ta.bbands(length=20, std=2)` then renamed to:\n",
    "    - `bb_lower`, `bb_middle`, `bb_upper`, `bb_bandwidth`, `bb_percent`\n",
    "  - Joined back to `df` on the **row index**.\n",
    "\n",
    "### Notes & gotchas\n",
    "- **Warm-up NaNs:** All rolling indicators need lookback data; the first `length-1` rows\n",
    "  per ticker will be `NaN`. Keep them or drop later.\n",
    "- **Index alignment:** `groupby(...).apply(pandas_ta.bbands(...))` returns a DataFrame\n",
    "  aligned to the original sub-index. The code resets the group level and `df.join(...)`\n",
    "  aligns on the remaining index. This is safe if your index hasn’t been disturbed.\n",
    "  If you ever see misalignment, set a stable index first:\n",
    "  ```python\n",
    "  df = df.sort_values([\"ticker_latest\",\"date\"]).set_index([\"ticker_latest\",\"date\"])\n",
    "  # ... compute features, then df = df.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7fad571d-637b-45d4-a208-79e7a746f105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Momentum\n",
    "import pandas_ta\n",
    "df['30_day_return'] = df.groupby('ticker_latest')['adj_close'].transform(lambda x: x.pct_change(periods=21))\n",
    "df['180_day_return'] = df.groupby('ticker_latest')['adj_close'].transform(lambda x: x.pct_change(periods=126))\n",
    "df['360_day_return'] = df.groupby('ticker_latest')['adj_close'].transform(lambda x: x.pct_change(periods=252))\n",
    "\n",
    "#rsi\n",
    "df['rsi'] = df.groupby(['ticker_latest'])['adj_close'].transform(lambda x: pandas_ta.rsi(close=x, length=14))\n",
    "df['rsi2'] = df.groupby(['ticker_latest'])['adj_close'].transform(lambda x: pandas_ta.rsi(close=x, length=9))\n",
    "df['rsi3'] = df.groupby(['ticker_latest'])['adj_close'].transform(lambda x: pandas_ta.rsi(close=x, length=3))\n",
    "\n",
    "#moving average\n",
    "df['sma'] = df.groupby('ticker_latest')['adj_close'].transform(lambda x: pandas_ta.sma(close=x, length=50))\n",
    "df['sma2'] = df.groupby('ticker_latest')['adj_close'].transform(lambda x: pandas_ta.sma(close=x, length=100))\n",
    "df['sma3'] = df.groupby('ticker_latest')['adj_close'].transform(lambda x: pandas_ta.sma(close=x, length=200))\n",
    "\n",
    "\n",
    "# Calculate Bollinger Bands\n",
    "bbands = df.groupby(['ticker_latest'])['adj_close'].apply(lambda x: pandas_ta.bbands(close=x, length=20, std=2))\n",
    "bbands.columns = ['bb_lower', 'bb_middle', 'bb_upper', 'bb_bandwidth', 'bb_percent']\n",
    "bbands = bbands.reset_index(level=0, drop=True)\n",
    "df = df.join(bbands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f486d232-0318-4420-98db-9f2574eaa922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker_latest</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>volume</th>\n",
       "      <th>company_sk</th>\n",
       "      <th>30_day_return</th>\n",
       "      <th>...</th>\n",
       "      <th>rsi2</th>\n",
       "      <th>rsi3</th>\n",
       "      <th>sma</th>\n",
       "      <th>sma2</th>\n",
       "      <th>sma3</th>\n",
       "      <th>bb_lower</th>\n",
       "      <th>bb_middle</th>\n",
       "      <th>bb_upper</th>\n",
       "      <th>bb_bandwidth</th>\n",
       "      <th>bb_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7475</th>\n",
       "      <td>2013-09-03</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>17.610714</td>\n",
       "      <td>17.878571</td>\n",
       "      <td>17.405357</td>\n",
       "      <td>17.449286</td>\n",
       "      <td>15.081167</td>\n",
       "      <td>331928800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7476</th>\n",
       "      <td>2013-09-04</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>17.841429</td>\n",
       "      <td>17.937143</td>\n",
       "      <td>17.724285</td>\n",
       "      <td>17.810356</td>\n",
       "      <td>15.393235</td>\n",
       "      <td>345032800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7477</th>\n",
       "      <td>2013-09-05</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>17.866072</td>\n",
       "      <td>17.881430</td>\n",
       "      <td>17.629999</td>\n",
       "      <td>17.688213</td>\n",
       "      <td>15.287665</td>\n",
       "      <td>236367600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7478</th>\n",
       "      <td>2013-09-06</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>17.801430</td>\n",
       "      <td>17.834999</td>\n",
       "      <td>17.498215</td>\n",
       "      <td>17.793571</td>\n",
       "      <td>15.378724</td>\n",
       "      <td>359525600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.550627</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12110</th>\n",
       "      <td>2013-09-09</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>18.035713</td>\n",
       "      <td>18.139999</td>\n",
       "      <td>17.981428</td>\n",
       "      <td>18.077499</td>\n",
       "      <td>15.624122</td>\n",
       "      <td>340687200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>89.467744</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15141</th>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>248.300003</td>\n",
       "      <td>256.640015</td>\n",
       "      <td>248.119995</td>\n",
       "      <td>256.079987</td>\n",
       "      <td>256.079987</td>\n",
       "      <td>105517400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.138639</td>\n",
       "      <td>...</td>\n",
       "      <td>79.781041</td>\n",
       "      <td>94.891962</td>\n",
       "      <td>224.731473</td>\n",
       "      <td>214.199038</td>\n",
       "      <td>221.304487</td>\n",
       "      <td>222.415230</td>\n",
       "      <td>235.787000</td>\n",
       "      <td>249.158770</td>\n",
       "      <td>11.342245</td>\n",
       "      <td>1.258800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15142</th>\n",
       "      <td>2025-09-23</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>255.880005</td>\n",
       "      <td>257.339996</td>\n",
       "      <td>253.580002</td>\n",
       "      <td>254.429993</td>\n",
       "      <td>254.429993</td>\n",
       "      <td>60275200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.117097</td>\n",
       "      <td>...</td>\n",
       "      <td>75.765967</td>\n",
       "      <td>83.461136</td>\n",
       "      <td>225.652403</td>\n",
       "      <td>214.623527</td>\n",
       "      <td>221.367679</td>\n",
       "      <td>222.117372</td>\n",
       "      <td>237.150500</td>\n",
       "      <td>252.183627</td>\n",
       "      <td>12.678133</td>\n",
       "      <td>1.074714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15143</th>\n",
       "      <td>2025-09-24</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>255.220001</td>\n",
       "      <td>255.740005</td>\n",
       "      <td>251.039993</td>\n",
       "      <td>252.309998</td>\n",
       "      <td>252.309998</td>\n",
       "      <td>42303700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.110715</td>\n",
       "      <td>...</td>\n",
       "      <td>70.628171</td>\n",
       "      <td>67.735497</td>\n",
       "      <td>226.521144</td>\n",
       "      <td>215.018635</td>\n",
       "      <td>221.418477</td>\n",
       "      <td>222.351446</td>\n",
       "      <td>238.300500</td>\n",
       "      <td>254.249553</td>\n",
       "      <td>13.385665</td>\n",
       "      <td>0.939195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15144</th>\n",
       "      <td>2025-09-25</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>253.210007</td>\n",
       "      <td>257.170013</td>\n",
       "      <td>251.710007</td>\n",
       "      <td>256.869995</td>\n",
       "      <td>256.869995</td>\n",
       "      <td>55202100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.120187</td>\n",
       "      <td>...</td>\n",
       "      <td>74.768444</td>\n",
       "      <td>79.933987</td>\n",
       "      <td>227.460109</td>\n",
       "      <td>215.538849</td>\n",
       "      <td>221.491925</td>\n",
       "      <td>222.178807</td>\n",
       "      <td>239.619499</td>\n",
       "      <td>257.060191</td>\n",
       "      <td>14.556989</td>\n",
       "      <td>0.994547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15145</th>\n",
       "      <td>2025-09-26</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>254.100006</td>\n",
       "      <td>257.600006</td>\n",
       "      <td>253.779999</td>\n",
       "      <td>255.460007</td>\n",
       "      <td>255.460007</td>\n",
       "      <td>46045700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.108334</td>\n",
       "      <td>...</td>\n",
       "      <td>71.273582</td>\n",
       "      <td>68.008258</td>\n",
       "      <td>228.373670</td>\n",
       "      <td>216.109406</td>\n",
       "      <td>221.559320</td>\n",
       "      <td>222.348456</td>\n",
       "      <td>240.764500</td>\n",
       "      <td>259.180543</td>\n",
       "      <td>15.297972</td>\n",
       "      <td>0.898987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3036 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date ticker_latest        open        high         low  \\\n",
       "7475   2013-09-03          AAPL   17.610714   17.878571   17.405357   \n",
       "7476   2013-09-04          AAPL   17.841429   17.937143   17.724285   \n",
       "7477   2013-09-05          AAPL   17.866072   17.881430   17.629999   \n",
       "7478   2013-09-06          AAPL   17.801430   17.834999   17.498215   \n",
       "12110  2013-09-09          AAPL   18.035713   18.139999   17.981428   \n",
       "...           ...           ...         ...         ...         ...   \n",
       "15141  2025-09-22          AAPL  248.300003  256.640015  248.119995   \n",
       "15142  2025-09-23          AAPL  255.880005  257.339996  253.580002   \n",
       "15143  2025-09-24          AAPL  255.220001  255.740005  251.039993   \n",
       "15144  2025-09-25          AAPL  253.210007  257.170013  251.710007   \n",
       "15145  2025-09-26          AAPL  254.100006  257.600006  253.779999   \n",
       "\n",
       "            close   adj_close     volume  company_sk  30_day_return  ...  \\\n",
       "7475    17.449286   15.081167  331928800         NaN            NaN  ...   \n",
       "7476    17.810356   15.393235  345032800         NaN            NaN  ...   \n",
       "7477    17.688213   15.287665  236367600         NaN            NaN  ...   \n",
       "7478    17.793571   15.378724  359525600         NaN            NaN  ...   \n",
       "12110   18.077499   15.624122  340687200         NaN            NaN  ...   \n",
       "...           ...         ...        ...         ...            ...  ...   \n",
       "15141  256.079987  256.079987  105517400         NaN       0.138639  ...   \n",
       "15142  254.429993  254.429993   60275200         NaN       0.117097  ...   \n",
       "15143  252.309998  252.309998   42303700         NaN       0.110715  ...   \n",
       "15144  256.869995  256.869995   55202100         NaN       0.120187  ...   \n",
       "15145  255.460007  255.460007   46045700         NaN       0.108334  ...   \n",
       "\n",
       "            rsi2       rsi3         sma        sma2        sma3    bb_lower  \\\n",
       "7475         NaN        NaN         NaN         NaN         NaN         NaN   \n",
       "7476         NaN        NaN         NaN         NaN         NaN         NaN   \n",
       "7477         NaN        NaN         NaN         NaN         NaN         NaN   \n",
       "7478         NaN  76.550627         NaN         NaN         NaN         NaN   \n",
       "12110        NaN  89.467744         NaN         NaN         NaN         NaN   \n",
       "...          ...        ...         ...         ...         ...         ...   \n",
       "15141  79.781041  94.891962  224.731473  214.199038  221.304487  222.415230   \n",
       "15142  75.765967  83.461136  225.652403  214.623527  221.367679  222.117372   \n",
       "15143  70.628171  67.735497  226.521144  215.018635  221.418477  222.351446   \n",
       "15144  74.768444  79.933987  227.460109  215.538849  221.491925  222.178807   \n",
       "15145  71.273582  68.008258  228.373670  216.109406  221.559320  222.348456   \n",
       "\n",
       "        bb_middle    bb_upper  bb_bandwidth  bb_percent  \n",
       "7475          NaN         NaN           NaN         NaN  \n",
       "7476          NaN         NaN           NaN         NaN  \n",
       "7477          NaN         NaN           NaN         NaN  \n",
       "7478          NaN         NaN           NaN         NaN  \n",
       "12110         NaN         NaN           NaN         NaN  \n",
       "...           ...         ...           ...         ...  \n",
       "15141  235.787000  249.158770     11.342245    1.258800  \n",
       "15142  237.150500  252.183627     12.678133    1.074714  \n",
       "15143  238.300500  254.249553     13.385665    0.939195  \n",
       "15144  239.619499  257.060191     14.556989    0.994547  \n",
       "15145  240.764500  259.180543     15.297972    0.898987  \n",
       "\n",
       "[3036 rows x 23 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.ticker_latest == 'AAPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "31331d55-bd54-49cf-8658-05be689b15aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('company_sk',axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2f55f925-afff-4c66-9c83-0c3f35c05c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('volume',axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2a622439-90d9-43c2-ba94-f258107ebbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tech = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "38abb63b-c917-4bce-ba0e-59d7b3736946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'ticker_latest', 'open', 'high', 'low', 'close', 'adj_close',\n",
       "       '30_day_return', '180_day_return', '360_day_return', 'rsi', 'rsi2',\n",
       "       'rsi3', 'sma', 'sma2', 'sma3', 'bb_lower', 'bb_middle', 'bb_upper',\n",
       "       'bb_bandwidth', 'bb_percent'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tech.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ed68e121-6d81-420b-9e8c-680c8d9f3029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting df has columns:\n",
    "# ['date','ticker_latest','open','high','low','close','adj_close',\n",
    "#  '30_day_return','180_day_return','360_day_return','rsi','rsi2','rsi3',\n",
    "#  'sma','sma2','sma3','bb_lower','bb_middle','bb_upper','bb_bandwidth','bb_percent']\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "rename_map = {\n",
    "    \"30_day_return\": \"ret_30d\",\n",
    "    \"180_day_return\": \"ret_180d\",\n",
    "    \"360_day_return\": \"ret_360d\",\n",
    "    \"rsi\":  \"rsi_14\",\n",
    "    \"rsi2\": \"rsi_9\",\n",
    "    \"rsi3\": \"rsi_3\",\n",
    "    \"sma\":  \"sma_50\",\n",
    "    \"sma2\": \"sma_100\",\n",
    "    \"sma3\": \"sma_200\",\n",
    "}\n",
    "\n",
    "tech = df.rename(columns=rename_map).copy()\n",
    "\n",
    "# normalize types / sort\n",
    "tech[\"date\"] = pd.to_datetime(tech[\"date\"], errors=\"coerce\").dt.date\n",
    "tech[\"ticker_latest\"] = tech[\"ticker_latest\"].astype(str).str.upper().str.strip()\n",
    "tech = tech.sort_values([\"ticker_latest\",\"date\"])\n",
    "\n",
    "# (optional) clip RSI to valid range\n",
    "for c in [\"rsi_14\",\"rsi_9\",\"rsi_3\"]:\n",
    "    if c in tech.columns:\n",
    "        tech[c] = tech[c].clip(lower=0, upper=100)\n",
    "\n",
    "# reorder to exactly what the loader expects\n",
    "required_cols = [\n",
    "    \"date\",\"ticker_latest\",\"adj_close\",\n",
    "    \"ret_30d\",\"ret_180d\",\"ret_360d\",\n",
    "    \"rsi_14\",\"rsi_9\",\"rsi_3\",\n",
    "    \"sma_50\",\"sma_100\",\"sma_200\",\n",
    "    \"bb_lower\",\"bb_middle\",\"bb_upper\",\"bb_bandwidth\",\"bb_percent\",\n",
    "]\n",
    "missing = [c for c in required_cols if c not in tech.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing expected columns after rename: {missing}\")\n",
    "\n",
    "tech = tech[required_cols]\n",
    "\n",
    "# now pass `tech` into your upsert block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56933af4-5f94-4bd2-8af2-e201769cb387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac60a7f8-4f1d-4f2b-b953-17f326121844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b0bcdf5c-5c0c-48c8-88a4-ab78b0b30f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upsert complete → sp500_prices_technicals_daily\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.dialects.postgresql import DATE, VARCHAR, DOUBLE_PRECISION, BIGINT\n",
    "import pandas as pd\n",
    "\n",
    "ENGINE_URL = \"postgresql://postgres:CSDBMS623@localhost:5432/SP500_ML\"\n",
    "TABLE      = \"sp500_prices_technicals_daily\"\n",
    "STAGING    = TABLE + \"_stg\"\n",
    "\n",
    "engine = create_engine(ENGINE_URL)\n",
    "\n",
    "# (optional) quick checks to avoid silent schema mismatches\n",
    "required = {\n",
    "    \"date\",\"ticker_latest\",\"adj_close\",\"ret_30d\",\"ret_180d\",\"ret_360d\",\n",
    "    \"rsi_14\",\"rsi_9\",\"rsi_3\",\"sma_50\",\"sma_100\",\"sma_200\",\n",
    "    \"bb_lower\",\"bb_middle\",\"bb_upper\",\"bb_bandwidth\",\"bb_percent\"\n",
    "}\n",
    "missing = required - set(tech.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"tech is missing columns: {sorted(missing)}\")\n",
    "\n",
    "# normalize types\n",
    "tech = tech.copy()\n",
    "tech[\"date\"] = pd.to_datetime(tech[\"date\"], errors=\"coerce\").dt.date\n",
    "tech[\"ticker_latest\"] = tech[\"ticker_latest\"].astype(str).str.upper().str.strip()\n",
    "\n",
    "# 1) ensure target table exists\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS \"{TABLE}\" (\n",
    "            date date NOT NULL,\n",
    "            ticker_latest varchar(16) NOT NULL,\n",
    "            adj_close double precision,\n",
    "            ret_30d double precision,\n",
    "            ret_180d double precision,\n",
    "            ret_360d double precision,\n",
    "            rsi_14 double precision,\n",
    "            rsi_9 double precision,\n",
    "            rsi_3 double precision,\n",
    "            sma_50 double precision,\n",
    "            sma_100 double precision,\n",
    "            sma_200 double precision,\n",
    "            bb_lower double precision,\n",
    "            bb_middle double precision,\n",
    "            bb_upper double precision,\n",
    "            bb_bandwidth double precision,\n",
    "            bb_percent double precision,\n",
    "            PRIMARY KEY (date, ticker_latest)\n",
    "        );\n",
    "    \"\"\"))\n",
    "\n",
    "# 2) stage the data\n",
    "tech.to_sql(\n",
    "    STAGING, engine, if_exists=\"replace\", index=False, method=\"multi\", chunksize=50_000,\n",
    "    dtype={\n",
    "        \"date\": DATE(),\n",
    "        \"ticker_latest\": VARCHAR(16),\n",
    "        \"adj_close\": DOUBLE_PRECISION(),\n",
    "        \"ret_30d\": DOUBLE_PRECISION(),\n",
    "        \"ret_180d\": DOUBLE_PRECISION(),\n",
    "        \"ret_360d\": DOUBLE_PRECISION(),\n",
    "        \"rsi_14\": DOUBLE_PRECISION(),\n",
    "        \"rsi_9\": DOUBLE_PRECISION(),\n",
    "        \"rsi_3\": DOUBLE_PRECISION(),\n",
    "        \"sma_50\": DOUBLE_PRECISION(),\n",
    "        \"sma_100\": DOUBLE_PRECISION(),\n",
    "        \"sma_200\": DOUBLE_PRECISION(),\n",
    "        \"bb_lower\": DOUBLE_PRECISION(),\n",
    "        \"bb_middle\": DOUBLE_PRECISION(),\n",
    "        \"bb_upper\": DOUBLE_PRECISION(),\n",
    "        \"bb_bandwidth\": DOUBLE_PRECISION(),\n",
    "        \"bb_percent\": DOUBLE_PRECISION(),\n",
    "    }\n",
    ")\n",
    "\n",
    "# 3) upsert + drop staging\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(f\"\"\"\n",
    "        INSERT INTO \"{TABLE}\" (\n",
    "            date, ticker_latest, adj_close,\n",
    "            ret_30d, ret_180d, ret_360d,\n",
    "            rsi_14, rsi_9, rsi_3,\n",
    "            sma_50, sma_100, sma_200,\n",
    "            bb_lower, bb_middle, bb_upper, bb_bandwidth, bb_percent\n",
    "        )\n",
    "        SELECT\n",
    "            s.date, s.ticker_latest, s.adj_close,\n",
    "            s.ret_30d, s.ret_180d, s.ret_360d,\n",
    "            s.rsi_14, s.rsi_9, s.rsi_3,\n",
    "            s.sma_50, s.sma_100, s.sma_200,\n",
    "            s.bb_lower, s.bb_middle, s.bb_upper, s.bb_bandwidth, s.bb_percent\n",
    "        FROM \"{STAGING}\" s\n",
    "        ON CONFLICT (date, ticker_latest) DO UPDATE SET\n",
    "            adj_close   = EXCLUDED.adj_close,\n",
    "            ret_30d     = EXCLUDED.ret_30d,\n",
    "            ret_180d    = EXCLUDED.ret_180d,\n",
    "            ret_360d    = EXCLUDED.ret_360d,\n",
    "            rsi_14      = EXCLUDED.rsi_14,\n",
    "            rsi_9       = EXCLUDED.rsi_9,\n",
    "            rsi_3       = EXCLUDED.rsi_3,\n",
    "            sma_50      = EXCLUDED.sma_50,\n",
    "            sma_100     = EXCLUDED.sma_100,\n",
    "            sma_200     = EXCLUDED.sma_200,\n",
    "            bb_lower    = EXCLUDED.bb_lower,\n",
    "            bb_middle   = EXCLUDED.bb_middle,\n",
    "            bb_upper    = EXCLUDED.bb_upper,\n",
    "            bb_bandwidth= EXCLUDED.bb_bandwidth,\n",
    "            bb_percent  = EXCLUDED.bb_percent;\n",
    "        DROP TABLE \"{STAGING}\";\n",
    "    \"\"\"))\n",
    "\n",
    "print(\"Upsert complete →\", TABLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae10ca5-a8a3-4a21-9b92-b9f175ccfff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37eae37-8c05-4f14-9f41-b188d5ebf6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "13326db7-df7c-4f92-b8d1-fd0c3cc7c4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_clean = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30453952-a148-4145-9e68-dc7f6464ec56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b385b853-4f6b-4b66-9e59-4c8375eedb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mm rows: 3036\n",
      "            date    mkt_1m    mkt_6m   mkt_12m\n",
      "3031  2025-09-22  0.050796  0.181064  0.191428\n",
      "3032  2025-09-23  0.029382  0.154198  0.165093\n",
      "3033  2025-09-24  0.030850  0.149104  0.164035\n",
      "3034  2025-09-25  0.021463  0.156248  0.154960\n",
      "3035  2025-09-26  0.025041  0.166931  0.158866\n"
     ]
    }
   ],
   "source": [
    "# --- Build `mm` (market momentum) with plain date ---\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# Use your prices range to choose a start date\n",
    "mkt_start = pd.to_datetime(prices_clean[\"date\"], errors=\"coerce\").min().date()\n",
    "\n",
    "gspc = yf.Ticker(\"^GSPC\").history(\n",
    "    start=mkt_start, auto_adjust=False, interval=\"1d\", actions=False\n",
    ").reset_index().rename(columns={\"Date\": \"date\"})\n",
    "\n",
    "# robust adjusted close detection\n",
    "adj_col = next((c for c in gspc.columns if \"adj\" in c.lower() and \"close\" in c.lower()), None)\n",
    "if adj_col is None:\n",
    "    raise ValueError(f\"Adjusted close not found in benchmark columns: {list(gspc.columns)}\")\n",
    "\n",
    "gspc[\"date\"] = pd.to_datetime(gspc[\"date\"], errors=\"coerce\").dt.date\n",
    "gspc = gspc.sort_values(\"date\").dropna(subset=[adj_col])\n",
    "\n",
    "# 1M/6M/12M windows ≈ 21/126/252 trading days\n",
    "WINS = {\"1m\": 21, \"6m\": 126, \"12m\": 252}\n",
    "for tag, W in WINS.items():\n",
    "    gspc[f\"mkt_{tag}\"] = gspc[adj_col].pct_change(W)\n",
    "\n",
    "mm = gspc[[\"date\", \"mkt_1m\", \"mkt_6m\", \"mkt_12m\"]].drop_duplicates(\"date\").reset_index(drop=True)\n",
    "\n",
    "print(\"mm rows:\", len(mm))\n",
    "print(mm.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3111c387-cf85-490d-86a4-e760092548d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upsert complete → sp500_market_momentum_daily\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.dialects.postgresql import DATE, DOUBLE_PRECISION\n",
    "\n",
    "ENGINE_URL = \"postgresql://postgres:CSDBMS623@localhost:5432/SP500_ML\"\n",
    "TABLE      = \"sp500_market_momentum_daily\"\n",
    "STAGING    = TABLE + \"_stg\"\n",
    "\n",
    "engine = create_engine(ENGINE_URL)\n",
    "\n",
    "# Ensure target table\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS \"{TABLE}\" (\n",
    "            date date PRIMARY KEY,\n",
    "            mkt_1m double precision,\n",
    "            mkt_6m double precision,\n",
    "            mkt_12m double precision\n",
    "        );\n",
    "    \"\"\"))\n",
    "\n",
    "# Stage\n",
    "mm.to_sql(\n",
    "    STAGING, engine, if_exists=\"replace\", index=False, method=\"multi\", chunksize=50_000,\n",
    "    dtype={\"date\": DATE(), \"mkt_1m\": DOUBLE_PRECISION(), \"mkt_6m\": DOUBLE_PRECISION(), \"mkt_12m\": DOUBLE_PRECISION()},\n",
    ")\n",
    "\n",
    "# Upsert\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(f\"\"\"\n",
    "        INSERT INTO \"{TABLE}\" (date, mkt_1m, mkt_6m, mkt_12m)\n",
    "        SELECT s.date, s.mkt_1m, s.mkt_6m, s.mkt_12m\n",
    "        FROM \"{STAGING}\" s\n",
    "        ON CONFLICT (date) DO UPDATE SET\n",
    "            mkt_1m  = EXCLUDED.mkt_1m,\n",
    "            mkt_6m  = EXCLUDED.mkt_6m,\n",
    "            mkt_12m = EXCLUDED.mkt_12m;\n",
    "        DROP TABLE \"{STAGING}\";\n",
    "    \"\"\"))\n",
    "\n",
    "print(\"Upsert complete →\", TABLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "803d0c8d-65a2-4753-b9f7-1303632e5dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "ENGINE_URL = \"postgresql://postgres:CSDBMS623@localhost:5432/SP500_ML\"\n",
    "engine = create_engine(ENGINE_URL)\n",
    "\n",
    "# ---------- 1) Daily prices ----------\n",
    "df_prices = pd.read_sql(\"\"\"\n",
    "    SELECT * FROM sp500_prices_daily_yahoo\n",
    "\"\"\", engine, parse_dates=[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0d7e4d21-4f01-4056-b94a-c410b2a6d51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker_latest</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>volume</th>\n",
       "      <th>company_sk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-09-03</td>\n",
       "      <td>A</td>\n",
       "      <td>33.648067</td>\n",
       "      <td>33.984264</td>\n",
       "      <td>33.347637</td>\n",
       "      <td>33.569386</td>\n",
       "      <td>30.354403</td>\n",
       "      <td>2188709</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-09-04</td>\n",
       "      <td>A</td>\n",
       "      <td>33.569386</td>\n",
       "      <td>34.306152</td>\n",
       "      <td>33.433475</td>\n",
       "      <td>34.241776</td>\n",
       "      <td>30.962395</td>\n",
       "      <td>3525756</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-09-05</td>\n",
       "      <td>A</td>\n",
       "      <td>34.206009</td>\n",
       "      <td>34.399143</td>\n",
       "      <td>34.034336</td>\n",
       "      <td>34.105865</td>\n",
       "      <td>30.839495</td>\n",
       "      <td>1835854</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-09-06</td>\n",
       "      <td>A</td>\n",
       "      <td>34.127323</td>\n",
       "      <td>34.184547</td>\n",
       "      <td>33.712444</td>\n",
       "      <td>33.927040</td>\n",
       "      <td>30.677790</td>\n",
       "      <td>1863394</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-09-09</td>\n",
       "      <td>A</td>\n",
       "      <td>34.041489</td>\n",
       "      <td>34.370529</td>\n",
       "      <td>33.826897</td>\n",
       "      <td>34.299000</td>\n",
       "      <td>31.014126</td>\n",
       "      <td>2333122</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797760</th>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>146.550003</td>\n",
       "      <td>146.710007</td>\n",
       "      <td>144.350006</td>\n",
       "      <td>144.649994</td>\n",
       "      <td>144.649994</td>\n",
       "      <td>2044700</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797761</th>\n",
       "      <td>2025-09-23</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>143.250000</td>\n",
       "      <td>146.179993</td>\n",
       "      <td>141.529999</td>\n",
       "      <td>142.610001</td>\n",
       "      <td>142.610001</td>\n",
       "      <td>3679500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797762</th>\n",
       "      <td>2025-09-24</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>141.619995</td>\n",
       "      <td>144.009995</td>\n",
       "      <td>140.539993</td>\n",
       "      <td>141.669998</td>\n",
       "      <td>141.669998</td>\n",
       "      <td>4224500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797763</th>\n",
       "      <td>2025-09-25</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>141.330002</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>139.339996</td>\n",
       "      <td>141.130005</td>\n",
       "      <td>141.130005</td>\n",
       "      <td>3058500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797764</th>\n",
       "      <td>2025-09-26</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>141.860001</td>\n",
       "      <td>143.789993</td>\n",
       "      <td>141.270004</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>2575200</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1797765 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date ticker_latest        open        high         low  \\\n",
       "0       2013-09-03             A   33.648067   33.984264   33.347637   \n",
       "1       2013-09-04             A   33.569386   34.306152   33.433475   \n",
       "2       2013-09-05             A   34.206009   34.399143   34.034336   \n",
       "3       2013-09-06             A   34.127323   34.184547   33.712444   \n",
       "4       2013-09-09             A   34.041489   34.370529   33.826897   \n",
       "...            ...           ...         ...         ...         ...   \n",
       "1797760 2025-09-22           ZTS  146.550003  146.710007  144.350006   \n",
       "1797761 2025-09-23           ZTS  143.250000  146.179993  141.529999   \n",
       "1797762 2025-09-24           ZTS  141.619995  144.009995  140.539993   \n",
       "1797763 2025-09-25           ZTS  141.330002  142.000000  139.339996   \n",
       "1797764 2025-09-26           ZTS  141.860001  143.789993  141.270004   \n",
       "\n",
       "              close   adj_close   volume  company_sk  \n",
       "0         33.569386   30.354403  2188709         NaN  \n",
       "1         34.241776   30.962395  3525756         NaN  \n",
       "2         34.105865   30.839495  1835854         NaN  \n",
       "3         33.927040   30.677790  1863394         NaN  \n",
       "4         34.299000   31.014126  2333122         NaN  \n",
       "...             ...         ...      ...         ...  \n",
       "1797760  144.649994  144.649994  2044700         NaN  \n",
       "1797761  142.610001  142.610001  3679500         NaN  \n",
       "1797762  141.669998  141.669998  4224500         NaN  \n",
       "1797763  141.130005  141.130005  3058500         NaN  \n",
       "1797764  143.500000  143.500000  2575200         NaN  \n",
       "\n",
       "[1797765 rows x 9 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "289ae699-0e5b-474c-b56b-4657599731b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def add_rolling_volatility(\n",
    "    df_prices: pd.DataFrame,\n",
    "    ticker_col: str = \"ticker_latest\",\n",
    "    date_col: str = \"date\",\n",
    "    price_col: str = \"adj_close\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds realized close-to-close volatility columns:\n",
    "      - vol_1m_ann, vol_6m_ann, vol_12m_ann  (annualized, using sqrt(252))\n",
    "      - vol_1m_win, vol_6m_win, vol_12m_win  (window-scale, using sqrt(window))\n",
    "    Windows use ~trading-day counts: 1m=21, 6m=126, 12m=252.\n",
    "    \"\"\"\n",
    "    df = df_prices.copy()\n",
    "\n",
    "    # basics\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "    df = df.sort_values([ticker_col, date_col])\n",
    "\n",
    "    # guard: non-positive prices -> NaN so logs don't explode\n",
    "    px = pd.to_numeric(df[price_col], errors=\"coerce\")\n",
    "    px = px.where(px > 0, np.nan)\n",
    "\n",
    "    # log returns per ticker\n",
    "    lr = np.log(px) - np.log(px.groupby(df[ticker_col]).shift(1))\n",
    "    df[\"log_ret\"] = lr\n",
    "\n",
    "    # windows in trading days\n",
    "    windows = {\n",
    "        \"1m\": 21,\n",
    "        \"6m\": 126,\n",
    "        \"12m\": 252,\n",
    "    }\n",
    "\n",
    "    # rolling std of daily log returns per ticker\n",
    "    g = df.groupby(ticker_col)[\"log_ret\"]\n",
    "    for tag, w in windows.items():\n",
    "        rstd = g.rolling(window=w, min_periods=w).std().reset_index(level=0, drop=True)\n",
    "\n",
    "        # annualized: multiply by sqrt(252)\n",
    "        df[f\"vol_{tag}_ann\"] = rstd * np.sqrt(252)\n",
    "\n",
    "        # window-scale (non-annualized): multiply by sqrt(w)\n",
    "        df[f\"vol_{tag}_win\"] = rstd * np.sqrt(w)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b4c43cda-8f6c-48e4-9289-c23b8796c262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              date ticker_latest  adj_close  vol_1m_ann  vol_6m_ann  \\\n",
      "0       2013-09-03             A  30.354403         NaN         NaN   \n",
      "1       2013-09-04             A  30.962395         NaN         NaN   \n",
      "2       2013-09-05             A  30.839495         NaN         NaN   \n",
      "3       2013-09-06             A  30.677790         NaN         NaN   \n",
      "4       2013-09-09             A  31.014126         NaN         NaN   \n",
      "5       2013-09-10             A  31.402214         NaN         NaN   \n",
      "6       2013-09-11             A  31.667398         NaN         NaN   \n",
      "7       2013-09-12             A  31.447481         NaN         NaN   \n",
      "8       2013-09-13             A  31.343987         NaN         NaN   \n",
      "9       2013-09-16             A  31.525116         NaN         NaN   \n",
      "10      2013-09-17             A  31.609182         NaN         NaN   \n",
      "11      2013-09-18             A  31.900240         NaN         NaN   \n",
      "12      2013-09-19             A  32.973938         NaN         NaN   \n",
      "13      2013-09-20             A  33.730713         NaN         NaN   \n",
      "14      2013-09-23             A  33.420235         NaN         NaN   \n",
      "15      2013-09-24             A  33.374958         NaN         NaN   \n",
      "16      2013-09-25             A  33.523712         NaN         NaN   \n",
      "1259111 2013-09-26             A  33.691891         NaN         NaN   \n",
      "1259112 2013-09-27             A  33.471466         NaN         NaN   \n",
      "1259113 2013-09-30             A  33.225124         NaN         NaN   \n",
      "1259114 2013-10-01             A  33.646507         NaN         NaN   \n",
      "1259115 2013-10-02             A  33.432568    0.179621         NaN   \n",
      "1259116 2013-10-03             A  33.101933    0.177402         NaN   \n",
      "1259117 2013-10-04             A  33.568722    0.179185         NaN   \n",
      "1259118 2013-10-07             A  33.147316    0.185709         NaN   \n",
      "1259119 2013-10-08             A  32.499012    0.200474         NaN   \n",
      "1259120 2013-10-09             A  32.382339    0.197863         NaN   \n",
      "1259121 2013-10-10             A  33.056572    0.207534         NaN   \n",
      "1259122 2013-10-11             A  33.361267    0.206230         NaN   \n",
      "1259123 2013-10-14             A  33.296429    0.205778         NaN   \n",
      "\n",
      "         vol_12m_ann  vol_1m_win  vol_6m_win  vol_12m_win  \n",
      "0                NaN         NaN         NaN          NaN  \n",
      "1                NaN         NaN         NaN          NaN  \n",
      "2                NaN         NaN         NaN          NaN  \n",
      "3                NaN         NaN         NaN          NaN  \n",
      "4                NaN         NaN         NaN          NaN  \n",
      "5                NaN         NaN         NaN          NaN  \n",
      "6                NaN         NaN         NaN          NaN  \n",
      "7                NaN         NaN         NaN          NaN  \n",
      "8                NaN         NaN         NaN          NaN  \n",
      "9                NaN         NaN         NaN          NaN  \n",
      "10               NaN         NaN         NaN          NaN  \n",
      "11               NaN         NaN         NaN          NaN  \n",
      "12               NaN         NaN         NaN          NaN  \n",
      "13               NaN         NaN         NaN          NaN  \n",
      "14               NaN         NaN         NaN          NaN  \n",
      "15               NaN         NaN         NaN          NaN  \n",
      "16               NaN         NaN         NaN          NaN  \n",
      "1259111          NaN         NaN         NaN          NaN  \n",
      "1259112          NaN         NaN         NaN          NaN  \n",
      "1259113          NaN         NaN         NaN          NaN  \n",
      "1259114          NaN         NaN         NaN          NaN  \n",
      "1259115          NaN    0.051852         NaN          NaN  \n",
      "1259116          NaN    0.051212         NaN          NaN  \n",
      "1259117          NaN    0.051726         NaN          NaN  \n",
      "1259118          NaN    0.053609         NaN          NaN  \n",
      "1259119          NaN    0.057872         NaN          NaN  \n",
      "1259120          NaN    0.057118         NaN          NaN  \n",
      "1259121          NaN    0.059910         NaN          NaN  \n",
      "1259122          NaN    0.059534         NaN          NaN  \n",
      "1259123          NaN    0.059403         NaN          NaN  \n"
     ]
    }
   ],
   "source": [
    "df_vol = add_rolling_volatility(df_prices)\n",
    "# Peek:\n",
    "cols = [\"date\",\"ticker_latest\",\"adj_close\",\n",
    "        \"vol_1m_ann\",\"vol_6m_ann\",\"vol_12m_ann\",\n",
    "        \"vol_1m_win\",\"vol_6m_win\",\"vol_12m_win\"]\n",
    "print(df_vol[cols].sort_values([\"ticker_latest\",\"date\"]).head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9fad6c1e-1bc8-4fea-898f-a40b13841800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 1) BUILD ANNUALIZED 1m/6m/12m VOL FROM DAILY PRICES ======\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def build_vol_df(\n",
    "    df_prices: pd.DataFrame,\n",
    "    ticker_col: str = \"ticker_latest\",\n",
    "    date_col: str = \"date\",\n",
    "    price_col: str = \"adj_close\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns only: ['symbol','date','vol_1m_ann','vol_6m_ann','vol_12m_ann'].\n",
    "    Vols are annualized realized vols from daily log returns of adj_close:\n",
    "      1m ~ 21d, 6m ~ 126d, 12m ~ 252d, annualize with sqrt(252).\n",
    "    \"\"\"\n",
    "    df = df_prices.copy()\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "    df = df.sort_values([ticker_col, date_col])\n",
    "\n",
    "    px = pd.to_numeric(df[price_col], errors=\"coerce\")\n",
    "    px = px.where(px > 0, np.nan)\n",
    "\n",
    "    # daily log-return per ticker\n",
    "    lr = np.log(px) - np.log(px.groupby(df[ticker_col]).shift(1))\n",
    "    df[\"_lr\"] = lr\n",
    "\n",
    "    windows = {\"1m\": 21, \"6m\": 126, \"12m\": 252}\n",
    "    g = df.groupby(ticker_col)[\"_lr\"]\n",
    "\n",
    "    for tag, w in windows.items():\n",
    "        rstd = g.rolling(window=w, min_periods=w).std().reset_index(level=0, drop=True)\n",
    "        df[f\"vol_{tag}_ann\"] = rstd * np.sqrt(252.0)\n",
    "\n",
    "    out = df[[ticker_col, date_col, \"vol_1m_ann\", \"vol_6m_ann\", \"vol_12m_ann\"]].rename(\n",
    "        columns={ticker_col: \"symbol\", date_col: \"date\"}\n",
    "    )\n",
    "    # Optional: drop rows where all three vols are NaN\n",
    "    # mask_all_nan = out[[\"vol_1m_ann\",\"vol_6m_ann\",\"vol_12m_ann\"]].isna().all(axis=1)\n",
    "    # out = out.loc[~mask_all_nan]\n",
    "    return out\n",
    "\n",
    "\n",
    "# ====== 2) POSTGRES UPSERT (auto-add columns, unique(symbol,date)) ======\n",
    "import math\n",
    "from typing import Sequence, Set, Dict\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy.types import Float, Text, DateTime\n",
    "\n",
    "PG_CONN_STR = \"postgresql://postgres:CSDBMS623@localhost:5432/SP500_ML\"\n",
    "SCHEMA      = \"public\"\n",
    "TABLE       = \"realized_vol_d\"\n",
    "CHUNK_ROWS  = 100_000\n",
    "\n",
    "def _get_engine(conn_str: str) -> Engine:\n",
    "    return create_engine(conn_str, pool_pre_ping=True)\n",
    "\n",
    "def ensure_table_and_indexes(engine: Engine, schema: str, table: str):\n",
    "    ddl = f'''\n",
    "    CREATE TABLE IF NOT EXISTS \"{schema}\".\"{table}\" (\n",
    "      symbol        TEXT,\n",
    "      date          TIMESTAMP,\n",
    "      vol_1m_ann    DOUBLE PRECISION,\n",
    "      vol_6m_ann    DOUBLE PRECISION,\n",
    "      vol_12m_ann   DOUBLE PRECISION\n",
    "    );'''\n",
    "    uq = f\"\"\"\n",
    "    DO $$\n",
    "    BEGIN\n",
    "      IF NOT EXISTS (\n",
    "        SELECT 1 FROM pg_constraint WHERE conname = '{table}_symbol_date_key'\n",
    "      ) THEN\n",
    "        ALTER TABLE \"{schema}\".\"{table}\"\n",
    "        ADD CONSTRAINT {table}_symbol_date_key UNIQUE (symbol, date);\n",
    "      END IF;\n",
    "    END$$;\"\"\"\n",
    "    idx1 = f'CREATE INDEX IF NOT EXISTS {table}_symbol_idx ON \"{schema}\".\"{table}\" (symbol);'\n",
    "    idx2 = f'CREATE INDEX IF NOT EXISTS {table}_date_idx   ON \"{schema}\".\"{table}\" (date);'\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(ddl)); conn.execute(text(uq)); conn.execute(text(idx1)); conn.execute(text(idx2))\n",
    "\n",
    "def _existing_columns(engine: Engine, schema: str, table: str) -> Set[str]:\n",
    "    sql = \"\"\"\n",
    "    SELECT lower(column_name) FROM information_schema.columns\n",
    "    WHERE table_schema=:schema AND table_name=:table\"\"\"\n",
    "    with engine.begin() as conn:\n",
    "        rows = conn.execute(text(sql), {\"schema\": schema, \"table\": table}).fetchall()\n",
    "    return {r[0] for r in rows}\n",
    "\n",
    "def ensure_missing_columns(engine: Engine, schema: str, table: str, df: pd.DataFrame):\n",
    "    have = _existing_columns(engine, schema, table)\n",
    "    add = [c for c in df.columns if c not in have]\n",
    "    if not add: return\n",
    "    alters = [f'ADD COLUMN IF NOT EXISTS {c} DOUBLE PRECISION' if c != \"date\" and c != \"symbol\"\n",
    "              else (f'ADD COLUMN IF NOT EXISTS {c} TIMESTAMP' if c==\"date\" else f'ADD COLUMN IF NOT EXISTS {c} TEXT')\n",
    "              for c in add]\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(f'ALTER TABLE \"{schema}\".\"{table}\" ' + \", \".join(alters) + \";\"))\n",
    "\n",
    "def _dtype_map(df: pd.DataFrame) -> Dict[str, object]:\n",
    "    d: Dict[str, object] = {}\n",
    "    for c in df.columns:\n",
    "        if c == \"date\":\n",
    "            d[c] = DateTime(timezone=False)\n",
    "        elif c == \"symbol\":\n",
    "            d[c] = Text()\n",
    "        else:\n",
    "            d[c] = Float()\n",
    "    return d\n",
    "\n",
    "def _to_sql_staging(engine: Engine, df: pd.DataFrame, schema: str, staging: str):\n",
    "    df.to_sql(\n",
    "        name=staging, con=engine, schema=schema,\n",
    "        if_exists=\"replace\", index=False,\n",
    "        dtype=_dtype_map(df), chunksize=50_000,\n",
    "    )\n",
    "\n",
    "def _merge_from_staging(engine: Engine, schema: str, table: str, staging: str, cols: Sequence[str]):\n",
    "    non_key = [c for c in cols if c not in (\"symbol\",\"date\")]\n",
    "    set_clause = \", \".join([f\"{c}=EXCLUDED.{c}\" for c in non_key]) or \"symbol=EXCLUDED.symbol\"\n",
    "    sql = f\"\"\"\n",
    "    INSERT INTO \"{schema}\".\"{table}\" ({\", \".join(cols)})\n",
    "    SELECT {\", \".join(cols)} FROM \"{schema}\".\"{staging}\"\n",
    "    ON CONFLICT (symbol, date) DO UPDATE SET {set_clause};\"\"\"\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(sql))\n",
    "\n",
    "def upsert_realized_vol_postgres(\n",
    "    df_vol: pd.DataFrame,\n",
    "    conn_str: str = PG_CONN_STR,\n",
    "    schema: str = SCHEMA,\n",
    "    table: str = TABLE,\n",
    "    chunk_rows: int = CHUNK_ROWS,\n",
    "):\n",
    "    if df_vol.empty:\n",
    "        print(\"vol DataFrame empty; nothing to ingest.\"); return\n",
    "    df = df_vol.copy()\n",
    "    # normalize cols\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df[\"symbol\"] = df[\"symbol\"].astype(str).str.upper()\n",
    "    df[\"date\"]   = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.tz_localize(None)\n",
    "\n",
    "    engine = _get_engine(conn_str)\n",
    "    ensure_table_and_indexes(engine, schema, table)\n",
    "    ensure_missing_columns(engine, schema, table, df)\n",
    "\n",
    "    # order columns\n",
    "    key_first = [c for c in (\"symbol\",\"date\") if c in df.columns]\n",
    "    rest = [c for c in df.columns if c not in key_first]\n",
    "    df = df[key_first + rest]\n",
    "\n",
    "    # chunked upsert\n",
    "    n = len(df); n_chunks = math.ceil(n / chunk_rows)\n",
    "    for i in range(n_chunks):\n",
    "        lo, hi = i*chunk_rows, min((i+1)*chunk_rows, n)\n",
    "        staging = f\"stg_{table}\"\n",
    "        _to_sql_staging(engine, df.iloc[lo:hi].copy(), schema, staging)\n",
    "        _merge_from_staging(engine, schema, table, staging, df.columns.tolist())\n",
    "        print(f\"Upserted rows {lo}–{hi} / {n}\")\n",
    "    print(\"✅ Realized volatility ingestion complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "93bc3e51-7144-4fb9-9173-089880c84fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted rows 0–100000 / 1797765\n",
      "Upserted rows 100000–200000 / 1797765\n",
      "Upserted rows 200000–300000 / 1797765\n",
      "Upserted rows 300000–400000 / 1797765\n",
      "Upserted rows 400000–500000 / 1797765\n",
      "Upserted rows 500000–600000 / 1797765\n",
      "Upserted rows 600000–700000 / 1797765\n",
      "Upserted rows 700000–800000 / 1797765\n",
      "Upserted rows 800000–900000 / 1797765\n",
      "Upserted rows 900000–1000000 / 1797765\n",
      "Upserted rows 1000000–1100000 / 1797765\n",
      "Upserted rows 1100000–1200000 / 1797765\n",
      "Upserted rows 1200000–1300000 / 1797765\n",
      "Upserted rows 1300000–1400000 / 1797765\n",
      "Upserted rows 1400000–1500000 / 1797765\n",
      "Upserted rows 1500000–1600000 / 1797765\n",
      "Upserted rows 1600000–1700000 / 1797765\n",
      "Upserted rows 1700000–1797765 / 1797765\n",
      "✅ Realized volatility ingestion complete.\n"
     ]
    }
   ],
   "source": [
    "# 1) Build vols (only symbol/date/annualized vols)\n",
    "df_vol = build_vol_df(df_prices)  # expects columns: date, ticker_latest, adj_close\n",
    "\n",
    "# 2) Ingest to Postgres\n",
    "upsert_realized_vol_postgres(df_vol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1c8733ed-1d8a-4d44-82f2-e3c336eecd17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>date</th>\n",
       "      <th>vol_1m_ann</th>\n",
       "      <th>vol_6m_ann</th>\n",
       "      <th>vol_12m_ann</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>2013-09-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>2013-09-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>2013-09-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>2013-09-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>2013-09-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797760</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>0.140623</td>\n",
       "      <td>0.273165</td>\n",
       "      <td>0.255497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797761</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>2025-09-23</td>\n",
       "      <td>0.140432</td>\n",
       "      <td>0.273799</td>\n",
       "      <td>0.255361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797762</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>2025-09-24</td>\n",
       "      <td>0.139628</td>\n",
       "      <td>0.273507</td>\n",
       "      <td>0.255412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797763</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>2025-09-25</td>\n",
       "      <td>0.139497</td>\n",
       "      <td>0.273136</td>\n",
       "      <td>0.255352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797764</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>2025-09-26</td>\n",
       "      <td>0.155272</td>\n",
       "      <td>0.273520</td>\n",
       "      <td>0.255970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1797765 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        symbol       date  vol_1m_ann  vol_6m_ann  vol_12m_ann\n",
       "0            A 2013-09-03         NaN         NaN          NaN\n",
       "1            A 2013-09-04         NaN         NaN          NaN\n",
       "2            A 2013-09-05         NaN         NaN          NaN\n",
       "3            A 2013-09-06         NaN         NaN          NaN\n",
       "4            A 2013-09-09         NaN         NaN          NaN\n",
       "...        ...        ...         ...         ...          ...\n",
       "1797760    ZTS 2025-09-22    0.140623    0.273165     0.255497\n",
       "1797761    ZTS 2025-09-23    0.140432    0.273799     0.255361\n",
       "1797762    ZTS 2025-09-24    0.139628    0.273507     0.255412\n",
       "1797763    ZTS 2025-09-25    0.139497    0.273136     0.255352\n",
       "1797764    ZTS 2025-09-26    0.155272    0.273520     0.255970\n",
       "\n",
       "[1797765 rows x 5 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b718452-193d-424b-8ea7-8f9ae60b7eca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
