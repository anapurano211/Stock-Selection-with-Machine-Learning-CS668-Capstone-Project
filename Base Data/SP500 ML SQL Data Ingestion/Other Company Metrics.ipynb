{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6f75bf2-dd71-4169-95bb-02046b125a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Universe size: 679\n"
     ]
    }
   ],
   "source": [
    "# !pip install sqlalchemy psycopg2-binary\n",
    "import os, pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "pg_user = os.getenv(\"PGUSER\", \"postgres\")\n",
    "pg_pass = os.getenv(\"PGPASSWORD\", \"CSDBMS623\")\n",
    "pg_host = os.getenv(\"PGHOST\", \"localhost\")\n",
    "pg_port = os.getenv(\"PGPORT\", \"5432\")\n",
    "pg_db   = os.getenv(\"PGDATABASE\", \"SP500_ML\")\n",
    "\n",
    "engine = create_engine(f\"postgresql+psycopg2://{pg_user}:{pg_pass}@{pg_host}:{pg_port}/{pg_db}\")\n",
    "\n",
    "# Use only the most recent membership date to define the current universe\n",
    "universe = pd.read_sql_query(\"\"\"\n",
    "    SELECT DISTINCT UPPER(TRIM(latest_ticker)) AS latest_ticker\n",
    "    FROM sp500_long_latest_profiles\n",
    "    WHERE latest_ticker IS NOT NULL\n",
    "\n",
    "\"\"\", engine)[\"latest_ticker\"].tolist()\n",
    "\n",
    "print(\"Universe size:\", len(universe))\n",
    "\n",
    "# Now run your Yahoo fetcher\n",
    "#prices_df = fetch_prices_for_universe(universe, checkpoint_path=\"prices_checkpoint.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d5c55ab-88d6-4b45-88a5-44446a74a70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: 5 tickers (1–5 of 679)\n",
      "Batch 2: 5 tickers (6–10 of 679)\n",
      "Batch 3: 5 tickers (11–15 of 679)\n",
      "Batch 4: 5 tickers (16–20 of 679)\n",
      "Batch 5: 5 tickers (21–25 of 679)\n",
      "Batch 6: 5 tickers (26–30 of 679)\n",
      "Batch 7: 5 tickers (31–35 of 679)\n",
      "Batch 8: 5 tickers (36–40 of 679)\n",
      "Batch 9: 5 tickers (41–45 of 679)\n",
      "Batch 10: 5 tickers (46–50 of 679)\n",
      "Batch 11: 5 tickers (51–55 of 679)\n",
      "Batch 12: 5 tickers (56–60 of 679)\n",
      "Batch 13: 5 tickers (61–65 of 679)\n",
      "Batch 14: 5 tickers (66–70 of 679)\n",
      "Batch 15: 5 tickers (71–75 of 679)\n",
      "Batch 16: 5 tickers (76–80 of 679)\n",
      "Batch 17: 5 tickers (81–85 of 679)\n",
      "Batch 18: 5 tickers (86–90 of 679)\n",
      "Batch 19: 5 tickers (91–95 of 679)\n",
      "Batch 20: 5 tickers (96–100 of 679)\n",
      "Batch 21: 5 tickers (101–105 of 679)\n",
      "Batch 22: 5 tickers (106–110 of 679)\n",
      "Batch 23: 5 tickers (111–115 of 679)\n",
      "Batch 24: 5 tickers (116–120 of 679)\n",
      "Batch 25: 5 tickers (121–125 of 679)\n",
      "Batch 26: 5 tickers (126–130 of 679)\n",
      "Batch 27: 5 tickers (131–135 of 679)\n",
      "Batch 28: 5 tickers (136–140 of 679)\n",
      "Batch 29: 5 tickers (141–145 of 679)\n",
      "Batch 30: 5 tickers (146–150 of 679)\n",
      "Batch 31: 5 tickers (151–155 of 679)\n",
      "Batch 32: 5 tickers (156–160 of 679)\n",
      "Batch 33: 5 tickers (161–165 of 679)\n",
      "Batch 34: 5 tickers (166–170 of 679)\n",
      "Batch 35: 5 tickers (171–175 of 679)\n",
      "Batch 36: 5 tickers (176–180 of 679)\n",
      "Batch 37: 5 tickers (181–185 of 679)\n",
      "Batch 38: 5 tickers (186–190 of 679)\n",
      "Batch 39: 5 tickers (191–195 of 679)\n",
      "Batch 40: 5 tickers (196–200 of 679)\n",
      "Batch 41: 5 tickers (201–205 of 679)\n",
      "Batch 42: 5 tickers (206–210 of 679)\n",
      "Batch 43: 5 tickers (211–215 of 679)\n",
      "Batch 44: 5 tickers (216–220 of 679)\n",
      "Batch 45: 5 tickers (221–225 of 679)\n",
      "Batch 46: 5 tickers (226–230 of 679)\n",
      "Batch 47: 5 tickers (231–235 of 679)\n",
      "Batch 48: 5 tickers (236–240 of 679)\n",
      "Batch 49: 5 tickers (241–245 of 679)\n",
      "Batch 50: 5 tickers (246–250 of 679)\n",
      "Batch 51: 5 tickers (251–255 of 679)\n",
      "Batch 52: 5 tickers (256–260 of 679)\n",
      "Batch 53: 5 tickers (261–265 of 679)\n",
      "Batch 54: 5 tickers (266–270 of 679)\n",
      "Batch 55: 5 tickers (271–275 of 679)\n",
      "Batch 56: 5 tickers (276–280 of 679)\n",
      "Batch 57: 5 tickers (281–285 of 679)\n",
      "Batch 58: 5 tickers (286–290 of 679)\n",
      "Batch 59: 5 tickers (291–295 of 679)\n",
      "Batch 60: 5 tickers (296–300 of 679)\n",
      "Batch 61: 5 tickers (301–305 of 679)\n",
      "Batch 62: 5 tickers (306–310 of 679)\n",
      "Batch 63: 5 tickers (311–315 of 679)\n",
      "Batch 64: 5 tickers (316–320 of 679)\n",
      "Batch 65: 5 tickers (321–325 of 679)\n",
      "Batch 66: 5 tickers (326–330 of 679)\n",
      "Batch 67: 5 tickers (331–335 of 679)\n",
      "Batch 68: 5 tickers (336–340 of 679)\n",
      "Batch 69: 5 tickers (341–345 of 679)\n",
      "Batch 70: 5 tickers (346–350 of 679)\n",
      "Batch 71: 5 tickers (351–355 of 679)\n",
      "Batch 72: 5 tickers (356–360 of 679)\n",
      "Batch 73: 5 tickers (361–365 of 679)\n",
      "Batch 74: 5 tickers (366–370 of 679)\n",
      "Batch 75: 5 tickers (371–375 of 679)\n",
      "Batch 76: 5 tickers (376–380 of 679)\n",
      "Batch 77: 5 tickers (381–385 of 679)\n",
      "Batch 78: 5 tickers (386–390 of 679)\n",
      "Batch 79: 5 tickers (391–395 of 679)\n",
      "Batch 80: 5 tickers (396–400 of 679)\n",
      "Batch 81: 5 tickers (401–405 of 679)\n",
      "Batch 82: 5 tickers (406–410 of 679)\n",
      "Batch 83: 5 tickers (411–415 of 679)\n",
      "Batch 84: 5 tickers (416–420 of 679)\n",
      "Batch 85: 5 tickers (421–425 of 679)\n",
      "Batch 86: 5 tickers (426–430 of 679)\n",
      "Batch 87: 5 tickers (431–435 of 679)\n",
      "Batch 88: 5 tickers (436–440 of 679)\n",
      "Batch 89: 5 tickers (441–445 of 679)\n",
      "Batch 90: 5 tickers (446–450 of 679)\n",
      "Batch 91: 5 tickers (451–455 of 679)\n",
      "Batch 92: 5 tickers (456–460 of 679)\n",
      "Batch 93: 5 tickers (461–465 of 679)\n",
      "Batch 94: 5 tickers (466–470 of 679)\n",
      "Batch 95: 5 tickers (471–475 of 679)\n",
      "Batch 96: 5 tickers (476–480 of 679)\n",
      "Batch 97: 5 tickers (481–485 of 679)\n",
      "Batch 98: 5 tickers (486–490 of 679)\n",
      "Batch 99: 5 tickers (491–495 of 679)\n",
      "Batch 100: 5 tickers (496–500 of 679)\n",
      "Batch 101: 5 tickers (501–505 of 679)\n",
      "Batch 102: 5 tickers (506–510 of 679)\n",
      "Batch 103: 5 tickers (511–515 of 679)\n",
      "Batch 104: 5 tickers (516–520 of 679)\n",
      "Batch 105: 5 tickers (521–525 of 679)\n",
      "Batch 106: 5 tickers (526–530 of 679)\n",
      "Batch 107: 5 tickers (531–535 of 679)\n",
      "Batch 108: 5 tickers (536–540 of 679)\n",
      "Batch 109: 5 tickers (541–545 of 679)\n",
      "Batch 110: 5 tickers (546–550 of 679)\n",
      "Batch 111: 5 tickers (551–555 of 679)\n",
      "Batch 112: 5 tickers (556–560 of 679)\n",
      "Batch 113: 5 tickers (561–565 of 679)\n",
      "Batch 114: 5 tickers (566–570 of 679)\n",
      "Batch 115: 5 tickers (571–575 of 679)\n",
      "Batch 116: 5 tickers (576–580 of 679)\n",
      "Batch 117: 5 tickers (581–585 of 679)\n",
      "Batch 118: 5 tickers (586–590 of 679)\n",
      "Batch 119: 5 tickers (591–595 of 679)\n",
      "Batch 120: 5 tickers (596–600 of 679)\n",
      "Batch 121: 5 tickers (601–605 of 679)\n",
      "Batch 122: 5 tickers (606–610 of 679)\n",
      "Batch 123: 5 tickers (611–615 of 679)\n",
      "Batch 124: 5 tickers (616–620 of 679)\n",
      "Batch 125: 5 tickers (621–625 of 679)\n",
      "Batch 126: 5 tickers (626–630 of 679)\n",
      "Batch 127: 5 tickers (631–635 of 679)\n",
      "Batch 128: 5 tickers (636–640 of 679)\n",
      "Batch 129: 5 tickers (641–645 of 679)\n",
      "Batch 130: 5 tickers (646–650 of 679)\n",
      "Batch 131: 5 tickers (651–655 of 679)\n",
      "Batch 132: 5 tickers (656–660 of 679)\n",
      "Batch 133: 5 tickers (661–665 of 679)\n",
      "Batch 134: 5 tickers (666–670 of 679)\n",
      "Batch 135: 5 tickers (671–675 of 679)\n",
      "Batch 136: 4 tickers (676–679 of 679)\n",
      "✅ Success: 678 ticker windows; rows: 2124458\n",
      "        symbol       date      marketCap\n",
      "2124446    ZTS 2025-09-11  67036511000.0\n",
      "2124447    ZTS 2025-09-12  65963819999.0\n",
      "2124448    ZTS 2025-09-15  65095875000.0\n",
      "2124449    ZTS 2025-09-16  65634446000.0\n",
      "2124450    ZTS 2025-09-17  65398543000.0\n",
      "2124451    ZTS 2025-09-18  65474210000.0\n",
      "2124452    ZTS 2025-09-19  64931188000.0\n",
      "2124453    ZTS 2025-09-22  64383715000.0\n",
      "2124454    ZTS 2025-09-23  63475711000.0\n",
      "2124455    ZTS 2025-09-24  63057316999.0\n",
      "2124456    ZTS 2025-09-25  62816963000.0\n",
      "2124457    ZTS 2025-09-26  63871850000.0\n"
     ]
    }
   ],
   "source": [
    "# ===================== FMP Historical Market Cap — Batched, 2012→Present =====================\n",
    "import time\n",
    "from datetime import date\n",
    "from typing import Iterable, List, Optional, Tuple, Union\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "FMP_HMC_BASE = \"https://financialmodelingprep.com/api/v3/historical-market-capitalization\"\n",
    "\n",
    "# -------- Robust GET with simple retries --------\n",
    "def _get_with_retries(\n",
    "    session: requests.Session,\n",
    "    url: str,\n",
    "    params: dict,\n",
    "    timeout: int = 30,\n",
    "    max_retries: int = 4,\n",
    "    base_sleep: float = 1.0,\n",
    "):\n",
    "    last = None\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        resp = session.get(url, params=params, timeout=timeout)\n",
    "        if resp.status_code == 200:\n",
    "            return resp\n",
    "        last = resp\n",
    "        # handle rate limits / transient\n",
    "        if resp.status_code in (429, 500, 502, 503, 504):\n",
    "            time.sleep(base_sleep * (2 ** (attempt - 1)))\n",
    "            continue\n",
    "        resp.raise_for_status()\n",
    "    if last is not None:\n",
    "        last.raise_for_status()\n",
    "    raise RuntimeError(\"Request failed without response.\")\n",
    "\n",
    "# -------- Normalize JSON -> list[dict] --------\n",
    "def _normalize_fmp_json(j):\n",
    "    if isinstance(j, list):\n",
    "        return j\n",
    "    if isinstance(j, dict):\n",
    "        for k in (\"error\", \"Error\", \"message\", \"Note\", \"Error Message\"):\n",
    "            if k in j and isinstance(j[k], str):\n",
    "                raise RuntimeError(f\"API message: {j[k]}\")\n",
    "        for k in (\"historical\", \"data\", \"results\", \"items\"):\n",
    "            if k in j and isinstance(j[k], list):\n",
    "                return j[k]\n",
    "        return [j]\n",
    "    raise RuntimeError(f\"Unexpected JSON type: {type(j)}\")\n",
    "\n",
    "# -------- Date windowing (≤5y per call) --------\n",
    "def _year_windows(start: pd.Timestamp, end: pd.Timestamp, years_per_call: int = 5) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Build [from, to] windows no longer than `years_per_call` years, inclusive.\n",
    "    Returns ISO date strings YYYY-MM-DD.\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    cur_start = start.normalize()\n",
    "    while cur_start <= end:\n",
    "        cur_end = min(cur_start + pd.DateOffset(years=years_per_call) - pd.Timedelta(days=1), end)\n",
    "        windows.append((cur_start.date().isoformat(), cur_end.date().isoformat()))\n",
    "        cur_start = cur_end + pd.Timedelta(days=1)\n",
    "    return windows\n",
    "\n",
    "# -------- One-ticker fetch over windows --------\n",
    "def fetch_market_cap_one(\n",
    "    ticker: str,\n",
    "    api_key: str,\n",
    "    start: Union[str, pd.Timestamp] = \"2012-01-01\",\n",
    "    end: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    session: Optional[requests.Session] = None,\n",
    "    timeout: int = 30,\n",
    "    per_call_limit: int = 200000,  # generous; API also enforces ~5y span per call\n",
    ") -> pd.DataFrame:\n",
    "    if session is None:\n",
    "        session = requests.Session()\n",
    "\n",
    "    if end is None:\n",
    "        end = pd.Timestamp.today().normalize()\n",
    "    start = pd.to_datetime(start)\n",
    "    end = pd.to_datetime(end)\n",
    "\n",
    "    frames: List[pd.DataFrame] = []\n",
    "    for f, t in _year_windows(start, end, years_per_call=5):\n",
    "        params = {\"from\": f, \"to\": t, \"apikey\": api_key, \"limit\": per_call_limit}\n",
    "        url = f\"{FMP_HMC_BASE}/{ticker.upper()}\"\n",
    "        r = _get_with_retries(session, url, params, timeout=timeout)\n",
    "        try:\n",
    "            data = r.json()\n",
    "        except ValueError as e:\n",
    "            raise RuntimeError(f\"Non-JSON response for {ticker}: {r.text[:300]}\") from e\n",
    "        records = _normalize_fmp_json(data)\n",
    "        if not records:\n",
    "            continue\n",
    "        df = pd.DataFrame.from_records(records)\n",
    "\n",
    "        # Harmonize keys; FMP uses 'marketCap'; guard for variants.\n",
    "        if \"marketCap\" not in df.columns and \"marketcap\" in df.columns:\n",
    "            df.rename(columns={\"marketcap\": \"marketCap\"}, inplace=True)\n",
    "\n",
    "        # Ensure required columns\n",
    "        if \"symbol\" not in df.columns:\n",
    "            df[\"symbol\"] = ticker.upper()\n",
    "        else:\n",
    "            df[\"symbol\"] = df[\"symbol\"].astype(str).str.upper().replace({\"\": ticker.upper()})\n",
    "        if \"date\" not in df.columns:\n",
    "            # some payloads may nest under 'historical'; handled in _normalize, but just in case:\n",
    "            raise RuntimeError(f\"'date' missing for {ticker} window {f}→{t}\")\n",
    "\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "        df[\"marketCap\"] = pd.to_numeric(df.get(\"marketCap\"), errors=\"coerce\")\n",
    "\n",
    "        frames.append(df[[\"symbol\", \"date\", \"marketCap\"]])\n",
    "\n",
    "    if not frames:\n",
    "        # Return empty frame with expected columns to keep pipeline stable\n",
    "        return pd.DataFrame(columns=[\"symbol\", \"date\", \"marketCap\"])\n",
    "\n",
    "    out = (\n",
    "        pd.concat(frames, ignore_index=True)\n",
    "          .dropna(subset=[\"date\"])\n",
    "          .drop_duplicates(subset=[\"symbol\", \"date\"], keep=\"last\")\n",
    "          .sort_values([\"symbol\", \"date\"])\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "    # Optional: use pandas nullable Float64 if you prefer\n",
    "    out[\"marketCap\"] = out[\"marketCap\"].astype(\"Float64\")\n",
    "    return out\n",
    "\n",
    "# -------- Multi-ticker orchestrator (batched) --------\n",
    "def fetch_market_caps(\n",
    "    tickers: Union[str, Iterable[str]],\n",
    "    api_key: str,\n",
    "    start: Union[str, pd.Timestamp] = \"2012-01-01\",\n",
    "    end: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    batch_size: int = 25,\n",
    "    sleep_between_batches: float = 1.0,\n",
    "    timeout: int = 30,\n",
    "    skip_errors: bool = True,\n",
    "    verbose: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    if isinstance(tickers, str):\n",
    "        tickers = [tickers]\n",
    "    tickers = [t.upper().strip() for t in tickers if str(t).strip()]\n",
    "\n",
    "    session = requests.Session()\n",
    "    frames: List[pd.DataFrame] = []\n",
    "    skipped: List[Tuple[str, str]] = []\n",
    "\n",
    "    total = len(tickers)\n",
    "    for i in range(0, total, batch_size):\n",
    "        batch = tickers[i:i + batch_size]\n",
    "        if verbose:\n",
    "            print(f\"Batch {i//batch_size + 1}: {len(batch)} tickers ({i+1}–{min(i+len(batch), total)} of {total})\")\n",
    "\n",
    "        for t in batch:\n",
    "            try:\n",
    "                df_t = fetch_market_cap_one(\n",
    "                    t, api_key=api_key, start=start, end=end,\n",
    "                    session=session, timeout=timeout\n",
    "                )\n",
    "                if not df_t.empty:\n",
    "                    frames.append(df_t)\n",
    "            except Exception as e:\n",
    "                if skip_errors:\n",
    "                    skipped.append((t, str(e)))\n",
    "                    if verbose:\n",
    "                        print(f\"  [skip] {t}: {e}\")\n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "        if i + batch_size < total:\n",
    "            time.sleep(sleep_between_batches)\n",
    "\n",
    "    if not frames:\n",
    "        if verbose:\n",
    "            print(\"No successful market-cap pulls.\")\n",
    "            if skipped:\n",
    "                print(f\"Skipped {len(skipped)} tickers. Examples: {skipped[:5]}\")\n",
    "        return pd.DataFrame(columns=[\"symbol\", \"date\", \"marketCap\"])\n",
    "\n",
    "    df_all = pd.concat(frames, ignore_index=True)\n",
    "    if verbose:\n",
    "        print(f\"✅ Success: {len(frames)} ticker windows; rows: {len(df_all)}\")\n",
    "        if skipped:\n",
    "            print(f\"⚠️ Skipped {len(skipped)} tickers.\")\n",
    "    return df_all\n",
    "\n",
    "# -------------------------- Example --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    API_KEY = \"\"\n",
    "    tickers = universe# or a longer list\n",
    "\n",
    "    df_mcap = fetch_market_caps(\n",
    "        tickers=tickers,\n",
    "        api_key=API_KEY,\n",
    "        start=\"2012-01-01\",\n",
    "        end=None,                 # defaults to today\n",
    "        batch_size=5,\n",
    "        sleep_between_batches=3.0,\n",
    "        skip_errors=True,\n",
    "        verbose=True,\n",
    "    )\n",
    "    print(df_mcap.tail(12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3980e283-b9bb-4b25-8ddd-f6c14fbe3bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== DB INGEST — Historical Market Cap (auto-add columns) =====================\n",
    "import math\n",
    "from typing import Sequence, Set, Dict\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy.types import BigInteger, Float, Text, DateTime\n",
    "\n",
    "# ---------- Configure ----------\n",
    "PG_CONN_STR = \"postgresql://postgres:CSDBMS623@localhost:5432/SP500_ML\"\n",
    "SCHEMA      = \"public\"\n",
    "TABLE       = \"market_caps_d\"   # daily series\n",
    "CHUNK_ROWS  = 25_000\n",
    "\n",
    "# ---------- Engine ----------\n",
    "def _get_engine(conn_str: str) -> Engine:\n",
    "    return create_engine(conn_str, pool_pre_ping=True)\n",
    "\n",
    "# ---------- Create base table + indexes ----------\n",
    "def ensure_table_and_indexes(engine: Engine, schema: str, table: str):\n",
    "    ddl = f'''\n",
    "    CREATE TABLE IF NOT EXISTS \"{schema}\".\"{table}\" (\n",
    "      symbol     TEXT,\n",
    "      date       TIMESTAMP,\n",
    "      marketcap  DOUBLE PRECISION\n",
    "    );\n",
    "    '''\n",
    "    uq  = f\"\"\"\n",
    "    DO $$\n",
    "    BEGIN\n",
    "      IF NOT EXISTS (\n",
    "        SELECT 1 FROM pg_constraint WHERE conname = '{table}_symbol_date_key'\n",
    "      ) THEN\n",
    "        ALTER TABLE \"{schema}\".\"{table}\"\n",
    "        ADD CONSTRAINT {table}_symbol_date_key UNIQUE (symbol, date);\n",
    "      END IF;\n",
    "    END$$;\n",
    "    \"\"\"\n",
    "    idx1 = f'CREATE INDEX IF NOT EXISTS {table}_symbol_idx ON \"{schema}\".\"{table}\" (symbol);'\n",
    "    idx2 = f'CREATE INDEX IF NOT EXISTS {table}_date_idx   ON \"{schema}\".\"{table}\" (date);'\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(ddl))\n",
    "        conn.execute(text(uq))\n",
    "        conn.execute(text(idx1))\n",
    "        conn.execute(text(idx2))\n",
    "\n",
    "# ---------- Introspection + migration ----------\n",
    "def _existing_columns(engine: Engine, schema: str, table: str) -> Set[str]:\n",
    "    sql = \"\"\"\n",
    "    SELECT lower(column_name) FROM information_schema.columns\n",
    "    WHERE table_schema = :schema AND table_name = :table\n",
    "    \"\"\"\n",
    "    with engine.begin() as conn:\n",
    "        rows = conn.execute(text(sql), {\"schema\": schema, \"table\": table}).fetchall()\n",
    "    return {r[0] for r in rows}\n",
    "\n",
    "def _infer_sql_type_from_series(s: pd.Series) -> str:\n",
    "    if pd.api.types.is_datetime64_any_dtype(s): return \"TIMESTAMP\"\n",
    "    if pd.api.types.is_integer_dtype(s):        return \"BIGINT\"\n",
    "    if pd.api.types.is_float_dtype(s):          return \"DOUBLE PRECISION\"\n",
    "    return \"TEXT\"\n",
    "\n",
    "def ensure_missing_columns(engine: Engine, schema: str, table: str, df: pd.DataFrame):\n",
    "    have = _existing_columns(engine, schema, table)\n",
    "    missing = [c for c in df.columns if c not in have]\n",
    "    if not missing: return\n",
    "    alters = []\n",
    "    for c in missing:\n",
    "        sql_t = _infer_sql_type_from_series(df[c])\n",
    "        alters.append(f'ADD COLUMN IF NOT EXISTS {c} {sql_t}')\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(f'ALTER TABLE \"{schema}\".\"{table}\" ' + \", \".join(alters) + \";\"))\n",
    "\n",
    "# ---------- Staging + merge ----------\n",
    "def _build_dtype_map(df: pd.DataFrame) -> Dict[str, object]:\n",
    "    dmap: Dict[str, object] = {}\n",
    "    for c in df.columns:\n",
    "        s = df[c]\n",
    "        if pd.api.types.is_datetime64_any_dtype(s):\n",
    "            dmap[c] = DateTime(timezone=False)\n",
    "        elif pd.api.types.is_integer_dtype(s):\n",
    "            dmap[c] = BigInteger()\n",
    "        elif pd.api.types.is_float_dtype(s):\n",
    "            dmap[c] = Float()\n",
    "        else:\n",
    "            dmap[c] = Text()\n",
    "    return dmap\n",
    "\n",
    "def _to_sql_staging(engine: Engine, df: pd.DataFrame, schema: str, staging: str):\n",
    "    df.to_sql(\n",
    "        name=staging,\n",
    "        con=engine,\n",
    "        schema=schema,\n",
    "        if_exists=\"replace\",\n",
    "        index=False,\n",
    "        dtype=_build_dtype_map(df),\n",
    "        chunksize=10_000,\n",
    "    )\n",
    "\n",
    "def _merge_from_staging(engine: Engine, schema: str, table: str, staging: str, cols: Sequence[str]):\n",
    "    non_key_cols = [c for c in cols if c not in (\"symbol\", \"date\")]\n",
    "    set_clause = \", \".join([f\"{c}=EXCLUDED.{c}\" for c in non_key_cols]) or \"symbol=EXCLUDED.symbol\"\n",
    "    sql = f\"\"\"\n",
    "    INSERT INTO \"{schema}\".\"{table}\" ({\", \".join(cols)})\n",
    "    SELECT {\", \".join(cols)} FROM \"{schema}\".\"{staging}\"\n",
    "    ON CONFLICT (symbol, date)\n",
    "    DO UPDATE SET {set_clause};\n",
    "    \"\"\"\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(sql))\n",
    "\n",
    "# ---------- Public API ----------\n",
    "def upsert_market_caps_postgres(\n",
    "    df: pd.DataFrame,\n",
    "    conn_str: str = PG_CONN_STR,\n",
    "    schema: str = SCHEMA,\n",
    "    table: str = TABLE,\n",
    "    chunk_rows: int = CHUNK_ROWS,\n",
    "):\n",
    "    \"\"\"\n",
    "    Upsert df_mcap -> Postgres:\n",
    "      - lowercase columns\n",
    "      - ensure table + unique(symbol,date) + indexes\n",
    "      - auto-add any extra columns from df\n",
    "      - stage & upsert in chunks\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"df_mcap is empty; nothing to ingest.\")\n",
    "        return\n",
    "\n",
    "    df = df.copy()\n",
    "    # normalize column names: symbol/date/marketCap -> symbol/date/marketcap\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "    # common cleanups\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.tz_localize(None)\n",
    "    if \"symbol\" in df.columns:\n",
    "        df[\"symbol\"] = df[\"symbol\"].astype(str).str.upper()\n",
    "\n",
    "    # prefer 'marketcap' name if 'marketCap' came through\n",
    "    if \"marketcap\" not in df.columns and \"marketcap\" in [c.lower() for c in df.columns]:\n",
    "        pass  # already normalized via lowercasing\n",
    "    elif \"marketcap\" not in df.columns and \"marketcap\" not in df.columns and \"marketcap\" not in df.columns:\n",
    "        # ensure the base column exists (if user has a different name)\n",
    "        # No-op: but you can map your custom column to 'marketcap' here if needed.\n",
    "        pass\n",
    "\n",
    "    engine = _get_engine(conn_str)\n",
    "    ensure_table_and_indexes(engine, schema, table)\n",
    "    ensure_missing_columns(engine, schema, table, df)\n",
    "\n",
    "    # keys first for readability\n",
    "    key_first = [c for c in (\"symbol\", \"date\") if c in df.columns]\n",
    "    rest = [c for c in df.columns if c not in key_first]\n",
    "    df = df[key_first + rest]\n",
    "\n",
    "    # chunked stage + merge\n",
    "    n = len(df)\n",
    "    n_chunks = math.ceil(n / chunk_rows)\n",
    "    for i in range(n_chunks):\n",
    "        lo, hi = i * chunk_rows, min((i + 1) * chunk_rows, n)\n",
    "        staging = f\"stg_{table}\"\n",
    "        chunk = df.iloc[lo:hi].copy()\n",
    "        _to_sql_staging(engine, chunk, schema, staging)\n",
    "        _merge_from_staging(engine, schema, table, staging, chunk.columns.tolist())\n",
    "        print(f\"Upserted rows {lo}–{hi} / {n}\")\n",
    "\n",
    "    print(\"✅ Market cap ingestion complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c58ab93-5f93-4fc9-8895-4294507fc751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted rows 0–25000 / 2124458\n",
      "Upserted rows 25000–50000 / 2124458\n",
      "Upserted rows 50000–75000 / 2124458\n",
      "Upserted rows 75000–100000 / 2124458\n",
      "Upserted rows 100000–125000 / 2124458\n",
      "Upserted rows 125000–150000 / 2124458\n",
      "Upserted rows 150000–175000 / 2124458\n",
      "Upserted rows 175000–200000 / 2124458\n",
      "Upserted rows 200000–225000 / 2124458\n",
      "Upserted rows 225000–250000 / 2124458\n",
      "Upserted rows 250000–275000 / 2124458\n",
      "Upserted rows 275000–300000 / 2124458\n",
      "Upserted rows 300000–325000 / 2124458\n",
      "Upserted rows 325000–350000 / 2124458\n",
      "Upserted rows 350000–375000 / 2124458\n",
      "Upserted rows 375000–400000 / 2124458\n",
      "Upserted rows 400000–425000 / 2124458\n",
      "Upserted rows 425000–450000 / 2124458\n",
      "Upserted rows 450000–475000 / 2124458\n",
      "Upserted rows 475000–500000 / 2124458\n",
      "Upserted rows 500000–525000 / 2124458\n",
      "Upserted rows 525000–550000 / 2124458\n",
      "Upserted rows 550000–575000 / 2124458\n",
      "Upserted rows 575000–600000 / 2124458\n",
      "Upserted rows 600000–625000 / 2124458\n",
      "Upserted rows 625000–650000 / 2124458\n",
      "Upserted rows 650000–675000 / 2124458\n",
      "Upserted rows 675000–700000 / 2124458\n",
      "Upserted rows 700000–725000 / 2124458\n",
      "Upserted rows 725000–750000 / 2124458\n",
      "Upserted rows 750000–775000 / 2124458\n",
      "Upserted rows 775000–800000 / 2124458\n",
      "Upserted rows 800000–825000 / 2124458\n",
      "Upserted rows 825000–850000 / 2124458\n",
      "Upserted rows 850000–875000 / 2124458\n",
      "Upserted rows 875000–900000 / 2124458\n",
      "Upserted rows 900000–925000 / 2124458\n",
      "Upserted rows 925000–950000 / 2124458\n",
      "Upserted rows 950000–975000 / 2124458\n",
      "Upserted rows 975000–1000000 / 2124458\n",
      "Upserted rows 1000000–1025000 / 2124458\n",
      "Upserted rows 1025000–1050000 / 2124458\n",
      "Upserted rows 1050000–1075000 / 2124458\n",
      "Upserted rows 1075000–1100000 / 2124458\n",
      "Upserted rows 1100000–1125000 / 2124458\n",
      "Upserted rows 1125000–1150000 / 2124458\n",
      "Upserted rows 1150000–1175000 / 2124458\n",
      "Upserted rows 1175000–1200000 / 2124458\n",
      "Upserted rows 1200000–1225000 / 2124458\n",
      "Upserted rows 1225000–1250000 / 2124458\n",
      "Upserted rows 1250000–1275000 / 2124458\n",
      "Upserted rows 1275000–1300000 / 2124458\n",
      "Upserted rows 1300000–1325000 / 2124458\n",
      "Upserted rows 1325000–1350000 / 2124458\n",
      "Upserted rows 1350000–1375000 / 2124458\n",
      "Upserted rows 1375000–1400000 / 2124458\n",
      "Upserted rows 1400000–1425000 / 2124458\n",
      "Upserted rows 1425000–1450000 / 2124458\n",
      "Upserted rows 1450000–1475000 / 2124458\n",
      "Upserted rows 1475000–1500000 / 2124458\n",
      "Upserted rows 1500000–1525000 / 2124458\n",
      "Upserted rows 1525000–1550000 / 2124458\n",
      "Upserted rows 1550000–1575000 / 2124458\n",
      "Upserted rows 1575000–1600000 / 2124458\n",
      "Upserted rows 1600000–1625000 / 2124458\n",
      "Upserted rows 1625000–1650000 / 2124458\n",
      "Upserted rows 1650000–1675000 / 2124458\n",
      "Upserted rows 1675000–1700000 / 2124458\n",
      "Upserted rows 1700000–1725000 / 2124458\n",
      "Upserted rows 1725000–1750000 / 2124458\n",
      "Upserted rows 1750000–1775000 / 2124458\n",
      "Upserted rows 1775000–1800000 / 2124458\n",
      "Upserted rows 1800000–1825000 / 2124458\n",
      "Upserted rows 1825000–1850000 / 2124458\n",
      "Upserted rows 1850000–1875000 / 2124458\n",
      "Upserted rows 1875000–1900000 / 2124458\n",
      "Upserted rows 1900000–1925000 / 2124458\n",
      "Upserted rows 1925000–1950000 / 2124458\n",
      "Upserted rows 1950000–1975000 / 2124458\n",
      "Upserted rows 1975000–2000000 / 2124458\n",
      "Upserted rows 2000000–2025000 / 2124458\n",
      "Upserted rows 2025000–2050000 / 2124458\n",
      "Upserted rows 2050000–2075000 / 2124458\n",
      "Upserted rows 2075000–2100000 / 2124458\n",
      "Upserted rows 2100000–2124458 / 2124458\n",
      "✅ Market cap ingestion complete.\n"
     ]
    }
   ],
   "source": [
    "upsert_market_caps_postgres(df_mcap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f2c5dc2-46b9-4349-a1a5-0e841682de10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: 5 tickers (1–5 of 679)\n",
      "Batch 2: 5 tickers (6–10 of 679)\n",
      "Batch 3: 5 tickers (11–15 of 679)\n",
      "Batch 4: 5 tickers (16–20 of 679)\n",
      "Batch 5: 5 tickers (21–25 of 679)\n",
      "Batch 6: 5 tickers (26–30 of 679)\n",
      "Batch 7: 5 tickers (31–35 of 679)\n",
      "Batch 8: 5 tickers (36–40 of 679)\n",
      "Batch 9: 5 tickers (41–45 of 679)\n",
      "Batch 10: 5 tickers (46–50 of 679)\n",
      "Batch 11: 5 tickers (51–55 of 679)\n",
      "Batch 12: 5 tickers (56–60 of 679)\n",
      "Batch 13: 5 tickers (61–65 of 679)\n",
      "Batch 14: 5 tickers (66–70 of 679)\n",
      "Batch 15: 5 tickers (71–75 of 679)\n",
      "Batch 16: 5 tickers (76–80 of 679)\n",
      "Batch 17: 5 tickers (81–85 of 679)\n",
      "Batch 18: 5 tickers (86–90 of 679)\n",
      "Batch 19: 5 tickers (91–95 of 679)\n",
      "Batch 20: 5 tickers (96–100 of 679)\n",
      "Batch 21: 5 tickers (101–105 of 679)\n",
      "Batch 22: 5 tickers (106–110 of 679)\n",
      "Batch 23: 5 tickers (111–115 of 679)\n",
      "Batch 24: 5 tickers (116–120 of 679)\n",
      "Batch 25: 5 tickers (121–125 of 679)\n",
      "Batch 26: 5 tickers (126–130 of 679)\n",
      "Batch 27: 5 tickers (131–135 of 679)\n",
      "Batch 28: 5 tickers (136–140 of 679)\n",
      "Batch 29: 5 tickers (141–145 of 679)\n",
      "Batch 30: 5 tickers (146–150 of 679)\n",
      "Batch 31: 5 tickers (151–155 of 679)\n",
      "Batch 32: 5 tickers (156–160 of 679)\n",
      "Batch 33: 5 tickers (161–165 of 679)\n",
      "Batch 34: 5 tickers (166–170 of 679)\n",
      "Batch 35: 5 tickers (171–175 of 679)\n",
      "Batch 36: 5 tickers (176–180 of 679)\n",
      "Batch 37: 5 tickers (181–185 of 679)\n",
      "Batch 38: 5 tickers (186–190 of 679)\n",
      "Batch 39: 5 tickers (191–195 of 679)\n",
      "Batch 40: 5 tickers (196–200 of 679)\n",
      "Batch 41: 5 tickers (201–205 of 679)\n",
      "Batch 42: 5 tickers (206–210 of 679)\n",
      "Batch 43: 5 tickers (211–215 of 679)\n",
      "Batch 44: 5 tickers (216–220 of 679)\n",
      "Batch 45: 5 tickers (221–225 of 679)\n",
      "Batch 46: 5 tickers (226–230 of 679)\n",
      "Batch 47: 5 tickers (231–235 of 679)\n",
      "Batch 48: 5 tickers (236–240 of 679)\n",
      "Batch 49: 5 tickers (241–245 of 679)\n",
      "Batch 50: 5 tickers (246–250 of 679)\n",
      "Batch 51: 5 tickers (251–255 of 679)\n",
      "Batch 52: 5 tickers (256–260 of 679)\n",
      "Batch 53: 5 tickers (261–265 of 679)\n",
      "Batch 54: 5 tickers (266–270 of 679)\n",
      "Batch 55: 5 tickers (271–275 of 679)\n",
      "Batch 56: 5 tickers (276–280 of 679)\n",
      "Batch 57: 5 tickers (281–285 of 679)\n",
      "Batch 58: 5 tickers (286–290 of 679)\n",
      "Batch 59: 5 tickers (291–295 of 679)\n",
      "Batch 60: 5 tickers (296–300 of 679)\n",
      "Batch 61: 5 tickers (301–305 of 679)\n",
      "Batch 62: 5 tickers (306–310 of 679)\n",
      "Batch 63: 5 tickers (311–315 of 679)\n",
      "Batch 64: 5 tickers (316–320 of 679)\n",
      "Batch 65: 5 tickers (321–325 of 679)\n",
      "Batch 66: 5 tickers (326–330 of 679)\n",
      "Batch 67: 5 tickers (331–335 of 679)\n",
      "Batch 68: 5 tickers (336–340 of 679)\n",
      "Batch 69: 5 tickers (341–345 of 679)\n",
      "Batch 70: 5 tickers (346–350 of 679)\n",
      "Batch 71: 5 tickers (351–355 of 679)\n",
      "Batch 72: 5 tickers (356–360 of 679)\n",
      "Batch 73: 5 tickers (361–365 of 679)\n",
      "Batch 74: 5 tickers (366–370 of 679)\n",
      "Batch 75: 5 tickers (371–375 of 679)\n",
      "Batch 76: 5 tickers (376–380 of 679)\n",
      "Batch 77: 5 tickers (381–385 of 679)\n",
      "Batch 78: 5 tickers (386–390 of 679)\n",
      "Batch 79: 5 tickers (391–395 of 679)\n",
      "Batch 80: 5 tickers (396–400 of 679)\n",
      "Batch 81: 5 tickers (401–405 of 679)\n",
      "Batch 82: 5 tickers (406–410 of 679)\n",
      "Batch 83: 5 tickers (411–415 of 679)\n",
      "Batch 84: 5 tickers (416–420 of 679)\n",
      "Batch 85: 5 tickers (421–425 of 679)\n",
      "Batch 86: 5 tickers (426–430 of 679)\n",
      "Batch 87: 5 tickers (431–435 of 679)\n",
      "Batch 88: 5 tickers (436–440 of 679)\n",
      "Batch 89: 5 tickers (441–445 of 679)\n",
      "Batch 90: 5 tickers (446–450 of 679)\n",
      "Batch 91: 5 tickers (451–455 of 679)\n",
      "Batch 92: 5 tickers (456–460 of 679)\n",
      "Batch 93: 5 tickers (461–465 of 679)\n",
      "Batch 94: 5 tickers (466–470 of 679)\n",
      "Batch 95: 5 tickers (471–475 of 679)\n",
      "Batch 96: 5 tickers (476–480 of 679)\n",
      "Batch 97: 5 tickers (481–485 of 679)\n",
      "Batch 98: 5 tickers (486–490 of 679)\n",
      "Batch 99: 5 tickers (491–495 of 679)\n",
      "Batch 100: 5 tickers (496–500 of 679)\n",
      "Batch 101: 5 tickers (501–505 of 679)\n",
      "Batch 102: 5 tickers (506–510 of 679)\n",
      "Batch 103: 5 tickers (511–515 of 679)\n",
      "Batch 104: 5 tickers (516–520 of 679)\n",
      "Batch 105: 5 tickers (521–525 of 679)\n",
      "Batch 106: 5 tickers (526–530 of 679)\n",
      "Batch 107: 5 tickers (531–535 of 679)\n",
      "Batch 108: 5 tickers (536–540 of 679)\n",
      "Batch 109: 5 tickers (541–545 of 679)\n",
      "Batch 110: 5 tickers (546–550 of 679)\n",
      "Batch 111: 5 tickers (551–555 of 679)\n",
      "Batch 112: 5 tickers (556–560 of 679)\n",
      "Batch 113: 5 tickers (561–565 of 679)\n",
      "Batch 114: 5 tickers (566–570 of 679)\n",
      "Batch 115: 5 tickers (571–575 of 679)\n",
      "Batch 116: 5 tickers (576–580 of 679)\n",
      "Batch 117: 5 tickers (581–585 of 679)\n",
      "Batch 118: 5 tickers (586–590 of 679)\n",
      "Batch 119: 5 tickers (591–595 of 679)\n",
      "Batch 120: 5 tickers (596–600 of 679)\n",
      "Batch 121: 5 tickers (601–605 of 679)\n",
      "Batch 122: 5 tickers (606–610 of 679)\n",
      "Batch 123: 5 tickers (611–615 of 679)\n",
      "Batch 124: 5 tickers (616–620 of 679)\n",
      "Batch 125: 5 tickers (621–625 of 679)\n",
      "Batch 126: 5 tickers (626–630 of 679)\n",
      "Batch 127: 5 tickers (631–635 of 679)\n",
      "Batch 128: 5 tickers (636–640 of 679)\n",
      "Batch 129: 5 tickers (641–645 of 679)\n",
      "Batch 130: 5 tickers (646–650 of 679)\n",
      "Batch 131: 5 tickers (651–655 of 679)\n",
      "Batch 132: 5 tickers (656–660 of 679)\n",
      "Batch 133: 5 tickers (661–665 of 679)\n",
      "Batch 134: 5 tickers (666–670 of 679)\n",
      "Batch 135: 5 tickers (671–675 of 679)\n",
      "Batch 136: 4 tickers (676–679 of 679)\n",
      "✅ Success: 676 ticker chunks; rows: 34027\n",
      "   symbol       date  enterpriseValue  stockPrice  numberOfShares  \\\n",
      "0       A 2012-01-31     9087760000.0       30.37     348000000.0   \n",
      "1       A 2012-04-30     8776680000.0       30.16     348000000.0   \n",
      "2       A 2012-07-31     9819240000.0       27.38     348000000.0   \n",
      "3       A 2012-10-31     8968520000.0       25.74     348000000.0   \n",
      "4       A 2013-01-31    11021940000.0       32.02     347000000.0   \n",
      "5       A 2013-04-30    10059350000.0       29.63     345000000.0   \n",
      "6       A 2013-07-31    11215610000.0       31.99     339000000.0   \n",
      "7       A 2013-10-31    12039300000.0        36.3     331000000.0   \n",
      "8       A 2014-01-31    13799140000.0       41.58     333000000.0   \n",
      "9       A 2014-04-30    12610120000.0       38.64     333000000.0   \n",
      "10      A 2014-07-31    13221740000.0       40.11     334000000.0   \n",
      "11      A 2014-10-31    12648020000.0       39.53     334000000.0   \n",
      "\n",
      "    marketCapitalization  minusCashAndCashEquivalents  addTotalDebt  \n",
      "0          10568760000.0                 3.662000e+09  2.181000e+09  \n",
      "1          10495680000.0                 3.896000e+09  2.177000e+09  \n",
      "2           9528240000.0                 1.923000e+09  2.214000e+09  \n",
      "3           8957520000.0                 2.351000e+09  2.362000e+09  \n",
      "4          11110940000.0                 2.450000e+09  2.361000e+09  \n",
      "5          10222350000.0                 2.519000e+09  2.356000e+09  \n",
      "6          10844610000.0                 2.330000e+09  2.701000e+09  \n",
      "7          12015300000.0                 2.675000e+09  2.699000e+09  \n",
      "8          13846140000.0                 2.742000e+09  2.695000e+09  \n",
      "9          12867120000.0                 2.950000e+09  2.693000e+09  \n",
      "10         13396740000.0                 2.391000e+09  2.216000e+09  \n",
      "11         13203020000.0                 2.218000e+09  1.663000e+09  \n"
     ]
    }
   ],
   "source": [
    "# ===================== FMP Enterprise Value — Batched, 2012→Present =====================\n",
    "import time\n",
    "from typing import Iterable, List, Optional, Tuple, Union\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "FMP_EV_BASE = \"https://financialmodelingprep.com/api/v3/enterprise-values\"\n",
    "\n",
    "# -------- Robust GET with simple retries --------\n",
    "def _get_with_retries(\n",
    "    session: requests.Session,\n",
    "    url: str,\n",
    "    params: dict,\n",
    "    timeout: int = 30,\n",
    "    max_retries: int = 4,\n",
    "    base_sleep: float = 1.0,\n",
    "):\n",
    "    last = None\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        resp = session.get(url, params=params, timeout=timeout)\n",
    "        if resp.status_code == 200:\n",
    "            return resp\n",
    "        last = resp\n",
    "        if resp.status_code in (429, 500, 502, 503, 504):\n",
    "            time.sleep(base_sleep * (2 ** (attempt - 1)))\n",
    "            continue\n",
    "        resp.raise_for_status()\n",
    "    if last is not None:\n",
    "        last.raise_for_status()\n",
    "    raise RuntimeError(\"Request failed without response.\")\n",
    "\n",
    "# -------- Normalize Enterprise Values payload -> list[dict] --------\n",
    "def _normalize_ev_json(j, ticker: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    FMP often returns:\n",
    "      { \"symbol\": \"AAPL\", \"enterpriseValues\": [ {...}, {...} ] }\n",
    "    but sometimes we may get a raw list of dicts already.\n",
    "    This function returns a flat list of dicts, each with 'symbol' set.\n",
    "    \"\"\"\n",
    "    if isinstance(j, list):\n",
    "        # already a list of records (rare for this endpoint)\n",
    "        out = []\n",
    "        for rec in j:\n",
    "            rec = dict(rec)\n",
    "            rec[\"symbol\"] = (str(rec.get(\"symbol\", ticker)).upper() or ticker.upper())\n",
    "            out.append(rec)\n",
    "        return out\n",
    "\n",
    "    if isinstance(j, dict):\n",
    "        # common case: {symbol, enterpriseValues: [...]}\n",
    "        symbol = str(j.get(\"symbol\", ticker)).upper()\n",
    "        evs = j.get(\"enterpriseValues\")\n",
    "        if isinstance(evs, list):\n",
    "            out = []\n",
    "            for rec in evs:\n",
    "                r = dict(rec)\n",
    "                r[\"symbol\"] = symbol\n",
    "                out.append(r)\n",
    "            return out\n",
    "\n",
    "        # fallback: any list-like under other keys\n",
    "        for k in (\"items\", \"data\", \"results\", \"historical\"):\n",
    "            if isinstance(j.get(k), list):\n",
    "                out = []\n",
    "                for rec in j[k]:\n",
    "                    r = dict(rec)\n",
    "                    r[\"symbol\"] = symbol\n",
    "                    out.append(r)\n",
    "                return out\n",
    "\n",
    "        # single record fallback\n",
    "        one = dict(j)\n",
    "        one[\"symbol\"] = symbol\n",
    "        return [one]\n",
    "\n",
    "    raise RuntimeError(f\"Unexpected JSON type from API: {type(j)}\")\n",
    "\n",
    "# -------- One-ticker fetch (quarterly by default) --------\n",
    "def fetch_enterprise_values_one(\n",
    "    ticker: str,\n",
    "    api_key: str,\n",
    "    period: str = \"quarter\",   # \"quarter\" or \"annual\"\n",
    "    start: Union[str, pd.Timestamp] = \"2012-01-01\",\n",
    "    end: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    session: Optional[requests.Session] = None,\n",
    "    timeout: int = 30,\n",
    "    per_call_limit: int = 5000,  # large to get full history\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pulls enterprise values for one ticker.\n",
    "    Filters rows to [start, end] after fetch (endpoint does not support from/to).\n",
    "    \"\"\"\n",
    "    if session is None:\n",
    "        session = requests.Session()\n",
    "    url = f\"{FMP_EV_BASE}/{ticker.upper()}\"\n",
    "    params = {\"period\": period, \"apikey\": api_key, \"limit\": per_call_limit}\n",
    "\n",
    "    r = _get_with_retries(session, url, params, timeout=timeout)\n",
    "    try:\n",
    "        data = r.json()\n",
    "    except ValueError as e:\n",
    "        raise RuntimeError(f\"Non-JSON response for {ticker}: {r.text[:300]}\") from e\n",
    "\n",
    "    rows = _normalize_ev_json(data, ticker)\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=[\"symbol\", \"date\", \"enterpriseValue\"])\n",
    "\n",
    "    df = pd.DataFrame.from_records(rows)\n",
    "\n",
    "    # robust symbol handling\n",
    "    if \"symbol\" in df.columns:\n",
    "        df[\"symbol\"] = df[\"symbol\"].astype(str)\n",
    "        mask_missing = df[\"symbol\"].isin([\"\", \"None\", \"nan\", \"NaN\"]) | df[\"symbol\"].isna()\n",
    "        df.loc[mask_missing, \"symbol\"] = ticker\n",
    "        df[\"symbol\"] = df[\"symbol\"].str.upper()\n",
    "    else:\n",
    "        df[\"symbol\"] = ticker.upper()\n",
    "\n",
    "    # date\n",
    "    if \"date\" not in df.columns:\n",
    "        raise RuntimeError(f\"'date' missing in enterprise-values payload for {ticker}.\")\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "\n",
    "    # normalize numeric fields if present\n",
    "    rename_map = {\n",
    "        \"marketcap\": \"marketCapitalization\",\n",
    "        \"stockprice\": \"stockPrice\",\n",
    "        \"numberofshares\": \"numberOfShares\",\n",
    "        \"enterprisevalue\": \"enterpriseValue\",\n",
    "    }\n",
    "    for k, v in rename_map.items():\n",
    "        if k in df.columns and v not in df.columns:\n",
    "            df.rename(columns={k: v}, inplace=True)\n",
    "\n",
    "    num_cols = [c for c in (\"enterpriseValue\",\"marketCapitalization\",\"stockPrice\",\"numberOfShares\") if c in df.columns]\n",
    "    # also try to coerce any other numeric-looking fields (debt/cash if present)\n",
    "    for c in list(df.columns):\n",
    "        if c in (\"symbol\",\"date\"): \n",
    "            continue\n",
    "        # attempt numeric coercion; if all NaN it stays float anyway\n",
    "        try:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"ignore\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    for c in num_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"Float64\")\n",
    "\n",
    "    # de-dup & sort\n",
    "    df = (\n",
    "        df.dropna(subset=[\"date\"])\n",
    "          .drop_duplicates(subset=[\"symbol\", \"date\"], keep=\"last\")\n",
    "          .sort_values([\"symbol\", \"date\"])\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # date filter 2012 -> present\n",
    "    start_ts = pd.to_datetime(start)\n",
    "    end_ts = pd.to_datetime(end) if end is not None else pd.Timestamp.today().normalize()\n",
    "    df = df[(df[\"date\"] >= start_ts) & (df[\"date\"] <= end_ts)]\n",
    "\n",
    "    # ensure the main columns exist even if NaN\n",
    "    if \"enterpriseValue\" not in df.columns:\n",
    "        df[\"enterpriseValue\"] = pd.Series(pd.NA, index=df.index, dtype=\"Float64\")\n",
    "\n",
    "    # keep a tidy base + any extras FMP returned\n",
    "    base = [\"symbol\", \"date\", \"enterpriseValue\"]\n",
    "    extras = [c for c in df.columns if c not in base]\n",
    "    df = df[base + extras]\n",
    "\n",
    "    return df\n",
    "\n",
    "# -------- Multi-ticker orchestrator (batched) --------\n",
    "def fetch_enterprise_values(\n",
    "    tickers: Union[str, Iterable[str]],\n",
    "    api_key: str,\n",
    "    start: Union[str, pd.Timestamp] = \"2012-01-01\",\n",
    "    end: Optional[Union[str, pd.Timestamp]] = None,\n",
    "    period: str = \"quarter\",\n",
    "    batch_size: int = 25,\n",
    "    sleep_between_batches: float = 1.0,\n",
    "    timeout: int = 30,\n",
    "    skip_errors: bool = True,\n",
    "    verbose: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    if isinstance(tickers, str):\n",
    "        tickers = [tickers]\n",
    "    tickers = [t.upper().strip() for t in tickers if str(t).strip()]\n",
    "\n",
    "    session = requests.Session()\n",
    "    frames: List[pd.DataFrame] = []\n",
    "    skipped: List[Tuple[str, str]] = []\n",
    "    total = len(tickers)\n",
    "\n",
    "    for i in range(0, total, batch_size):\n",
    "        batch = tickers[i:i + batch_size]\n",
    "        if verbose:\n",
    "            print(f\"Batch {i//batch_size + 1}: {len(batch)} tickers ({i+1}–{min(i+len(batch), total)} of {total})\")\n",
    "\n",
    "        for t in batch:\n",
    "            try:\n",
    "                df_t = fetch_enterprise_values_one(\n",
    "                    t, api_key=api_key, period=period,\n",
    "                    start=start, end=end,\n",
    "                    session=session, timeout=timeout\n",
    "                )\n",
    "                if not df_t.empty:\n",
    "                    frames.append(df_t)\n",
    "            except Exception as e:\n",
    "                if skip_errors:\n",
    "                    skipped.append((t, str(e)))\n",
    "                    if verbose:\n",
    "                        print(f\"  [skip] {t}: {e}\")\n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "        if i + batch_size < total:\n",
    "            time.sleep(sleep_between_batches)\n",
    "\n",
    "    if not frames:\n",
    "        if verbose:\n",
    "            print(\"No successful enterprise-value pulls.\")\n",
    "            if skipped:\n",
    "                print(f\"Skipped {len(skipped)} tickers. Examples: {skipped[:5]}\")\n",
    "        return pd.DataFrame(columns=[\"symbol\", \"date\", \"enterpriseValue\"])\n",
    "\n",
    "    df_all = pd.concat(frames, ignore_index=True)\n",
    "    if verbose:\n",
    "        print(f\"✅ Success: {len(frames)} ticker chunks; rows: {len(df_all)}\")\n",
    "        if skipped:\n",
    "            print(f\"⚠️ Skipped {len(skipped)} tickers.\")\n",
    "    return df_all\n",
    "\n",
    "# -------------------------- Example --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    API_KEY = \"\"\n",
    "    tickers = universe # or a longer list\n",
    "\n",
    "    df_ev = fetch_enterprise_values(\n",
    "        tickers=tickers,\n",
    "        api_key=API_KEY,\n",
    "        start=\"2012-01-01\",\n",
    "        end=None,           # defaults to today\n",
    "        period=\"quarter\",   # or \"annual\"\n",
    "        batch_size=5,\n",
    "        sleep_between_batches=3.0,\n",
    "        skip_errors=True,\n",
    "        verbose=True,\n",
    "    )\n",
    "    print(df_ev.head(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f32c7854-d303-4b83-9dbb-a663b137b961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== DB INGEST — Enterprise Values (auto-add columns) =====================\n",
    "import math\n",
    "from typing import Sequence, Set, Dict\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy.types import BigInteger, Float, Text, DateTime\n",
    "\n",
    "# ---------- Configure ----------\n",
    "PG_CONN_STR = \"postgresql://postgres:CSDBMS623@localhost:5432/SP500_ML\"\n",
    "SCHEMA      = \"public\"\n",
    "TABLE       = \"enterprise_values_q\"   # quarterly EV series\n",
    "CHUNK_ROWS  = 25_000\n",
    "\n",
    "# ---------- Engine ----------\n",
    "def _get_engine(conn_str: str) -> Engine:\n",
    "    return create_engine(conn_str, pool_pre_ping=True)\n",
    "\n",
    "# ---------- Create base table + indexes ----------\n",
    "def ensure_table_and_indexes(engine: Engine, schema: str, table: str):\n",
    "    ddl = f'''\n",
    "    CREATE TABLE IF NOT EXISTS \"{schema}\".\"{table}\" (\n",
    "      symbol          TEXT,\n",
    "      date            TIMESTAMP,\n",
    "      enterprisevalue DOUBLE PRECISION\n",
    "    );\n",
    "    '''\n",
    "    uq  = f\"\"\"\n",
    "    DO $$\n",
    "    BEGIN\n",
    "      IF NOT EXISTS (\n",
    "        SELECT 1 FROM pg_constraint WHERE conname = '{table}_symbol_date_key'\n",
    "      ) THEN\n",
    "        ALTER TABLE \"{schema}\".\"{table}\"\n",
    "        ADD CONSTRAINT {table}_symbol_date_key UNIQUE (symbol, date);\n",
    "      END IF;\n",
    "    END$$;\n",
    "    \"\"\"\n",
    "    idx1 = f'CREATE INDEX IF NOT EXISTS {table}_symbol_idx ON \"{schema}\".\"{table}\" (symbol);'\n",
    "    idx2 = f'CREATE INDEX IF NOT EXISTS {table}_date_idx   ON \"{schema}\".\"{table}\" (date);'\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(ddl))\n",
    "        conn.execute(text(uq))\n",
    "        conn.execute(text(idx1))\n",
    "        conn.execute(text(idx2))\n",
    "\n",
    "# ---------- Introspection + migration ----------\n",
    "def _existing_columns(engine: Engine, schema: str, table: str) -> Set[str]:\n",
    "    sql = \"\"\"\n",
    "    SELECT lower(column_name) FROM information_schema.columns\n",
    "    WHERE table_schema = :schema AND table_name = :table\n",
    "    \"\"\"\n",
    "    with engine.begin() as conn:\n",
    "        rows = conn.execute(text(sql), {\"schema\": schema, \"table\": table}).fetchall()\n",
    "    return {r[0] for r in rows}\n",
    "\n",
    "def _infer_sql_type_from_series(s: pd.Series) -> str:\n",
    "    if pd.api.types.is_datetime64_any_dtype(s): return \"TIMESTAMP\"\n",
    "    if pd.api.types.is_integer_dtype(s):        return \"BIGINT\"\n",
    "    if pd.api.types.is_float_dtype(s):          return \"DOUBLE PRECISION\"  # covers float64 & pandas Float64\n",
    "    return \"TEXT\"\n",
    "\n",
    "def ensure_missing_columns(engine: Engine, schema: str, table: str, df: pd.DataFrame):\n",
    "    have = _existing_columns(engine, schema, table)\n",
    "    missing = [c for c in df.columns if c not in have]\n",
    "    if not missing: return\n",
    "    alters = []\n",
    "    for c in missing:\n",
    "        sql_t = _infer_sql_type_from_series(df[c])\n",
    "        alters.append(f'ADD COLUMN IF NOT EXISTS {c} {sql_t}')\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(f'ALTER TABLE \"{schema}\".\"{table}\" ' + \", \".join(alters) + \";\"))\n",
    "\n",
    "# ---------- Staging + merge ----------\n",
    "def _build_dtype_map(df: pd.DataFrame) -> Dict[str, object]:\n",
    "    dmap: Dict[str, object] = {}\n",
    "    for c in df.columns:\n",
    "        s = df[c]\n",
    "        if pd.api.types.is_datetime64_any_dtype(s):\n",
    "            dmap[c] = DateTime(timezone=False)\n",
    "        elif pd.api.types.is_integer_dtype(s):\n",
    "            dmap[c] = BigInteger()\n",
    "        elif pd.api.types.is_float_dtype(s):\n",
    "            dmap[c] = Float()\n",
    "        else:\n",
    "            dmap[c] = Text()\n",
    "    return dmap\n",
    "\n",
    "def _to_sql_staging(engine: Engine, df: pd.DataFrame, schema: str, staging: str):\n",
    "    df.to_sql(\n",
    "        name=staging,\n",
    "        con=engine,\n",
    "        schema=schema,\n",
    "        if_exists=\"replace\",\n",
    "        index=False,\n",
    "        dtype=_build_dtype_map(df),\n",
    "        chunksize=10_000,\n",
    "    )\n",
    "\n",
    "def _merge_from_staging(engine: Engine, schema: str, table: str, staging: str, cols: Sequence[str]):\n",
    "    non_key_cols = [c for c in cols if c not in (\"symbol\", \"date\")]\n",
    "    set_clause = \", \".join([f\"{c}=EXCLUDED.{c}\" for c in non_key_cols]) or \"symbol=EXCLUDED.symbol\"\n",
    "    sql = f\"\"\"\n",
    "    INSERT INTO \"{schema}\".\"{table}\" ({\", \".join(cols)})\n",
    "    SELECT {\", \".join(cols)} FROM \"{schema}\".\"{staging}\"\n",
    "    ON CONFLICT (symbol, date)\n",
    "    DO UPDATE SET {set_clause};\n",
    "    \"\"\"\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(sql))\n",
    "\n",
    "# ---------- Public API ----------\n",
    "def upsert_enterprise_values_postgres(\n",
    "    df: pd.DataFrame,\n",
    "    conn_str: str = PG_CONN_STR,\n",
    "    schema: str = SCHEMA,\n",
    "    table: str = TABLE,\n",
    "    chunk_rows: int = CHUNK_ROWS,\n",
    "):\n",
    "    \"\"\"\n",
    "    Upsert df_ev -> Postgres:\n",
    "      - lowercase columns\n",
    "      - ensure table + unique(symbol,date) + indexes\n",
    "      - auto-add any extra columns from df_ev\n",
    "      - stage & upsert in chunks\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"df_ev is empty; nothing to ingest.\")\n",
    "        return\n",
    "\n",
    "    df = df.copy()\n",
    "    # normalize column names to lowercase\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "    # canonicalize a few expected names\n",
    "    rename_map = {\n",
    "        \"enterprisevalue\": \"enterprisevalue\",\n",
    "        \"marketcapitalization\": \"marketcapitalization\",\n",
    "        \"stockprice\": \"stockprice\",\n",
    "        \"numberofshares\": \"numberofshares\",\n",
    "        \"minuscashandcashequivalents\": \"minuscashandcashequivalents\",\n",
    "        \"addtotaldebt\": \"addtotaldebt\",\n",
    "    }\n",
    "    # (lowercasing already handled; above is just clarity)\n",
    "\n",
    "    # clean types\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.tz_localize(None)\n",
    "    if \"symbol\" in df.columns:\n",
    "        df[\"symbol\"] = df[\"symbol\"].astype(str).str.upper()\n",
    "\n",
    "    engine = _get_engine(conn_str)\n",
    "    ensure_table_and_indexes(engine, schema, table)\n",
    "    ensure_missing_columns(engine, schema, table, df)\n",
    "\n",
    "    # keys first for readability\n",
    "    key_first = [c for c in (\"symbol\", \"date\") if c in df.columns]\n",
    "    rest = [c for c in df.columns if c not in key_first]\n",
    "    df = df[key_first + rest]\n",
    "\n",
    "    # chunked stage + merge\n",
    "    n = len(df)\n",
    "    n_chunks = math.ceil(n / chunk_rows)\n",
    "    for i in range(n_chunks):\n",
    "        lo, hi = i * chunk_rows, min((i + 1) * chunk_rows, n)\n",
    "        staging = f\"stg_{table}\"\n",
    "        chunk = df.iloc[lo:hi].copy()\n",
    "        _to_sql_staging(engine, chunk, schema, staging)\n",
    "        _merge_from_staging(engine, schema, table, staging, chunk.columns.tolist())\n",
    "        print(f\"Upserted rows {lo}–{hi} / {n}\")\n",
    "\n",
    "    print(\"✅ Enterprise values ingestion complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "757a7822-07cf-481d-b1e3-1da93b6542cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted rows 0–25000 / 34027\n",
      "Upserted rows 25000–34027 / 34027\n",
      "✅ Enterprise values ingestion complete.\n"
     ]
    }
   ],
   "source": [
    "# df_ev has columns like:\n",
    "# ['symbol','date','enterpriseValue','stockPrice','numberOfShares',\n",
    "#  'marketCapitalization','minusCashAndCashEquivalents','addTotalDebt']\n",
    "upsert_enterprise_values_postgres(df_ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53614ebd-b492-4641-9d6c-044e2a55785b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9cc7cfa4-7e5d-41d0-a1a8-75a39822e077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: 5 tickers (1–5 of 679)\n",
      "Batch 2: 5 tickers (6–10 of 679)\n",
      "Batch 3: 5 tickers (11–15 of 679)\n",
      "Batch 4: 5 tickers (16–20 of 679)\n",
      "Batch 5: 5 tickers (21–25 of 679)\n",
      "Batch 6: 5 tickers (26–30 of 679)\n",
      "Batch 7: 5 tickers (31–35 of 679)\n",
      "Batch 8: 5 tickers (36–40 of 679)\n",
      "Batch 9: 5 tickers (41–45 of 679)\n",
      "Batch 10: 5 tickers (46–50 of 679)\n",
      "Batch 11: 5 tickers (51–55 of 679)\n",
      "Batch 12: 5 tickers (56–60 of 679)\n",
      "Batch 13: 5 tickers (61–65 of 679)\n",
      "Batch 14: 5 tickers (66–70 of 679)\n",
      "Batch 15: 5 tickers (71–75 of 679)\n",
      "Batch 16: 5 tickers (76–80 of 679)\n",
      "Batch 17: 5 tickers (81–85 of 679)\n",
      "Batch 18: 5 tickers (86–90 of 679)\n",
      "Batch 19: 5 tickers (91–95 of 679)\n",
      "Batch 20: 5 tickers (96–100 of 679)\n",
      "Batch 21: 5 tickers (101–105 of 679)\n",
      "Batch 22: 5 tickers (106–110 of 679)\n",
      "Batch 23: 5 tickers (111–115 of 679)\n",
      "Batch 24: 5 tickers (116–120 of 679)\n",
      "Batch 25: 5 tickers (121–125 of 679)\n",
      "Batch 26: 5 tickers (126–130 of 679)\n",
      "Batch 27: 5 tickers (131–135 of 679)\n",
      "Batch 28: 5 tickers (136–140 of 679)\n",
      "Batch 29: 5 tickers (141–145 of 679)\n",
      "Batch 30: 5 tickers (146–150 of 679)\n",
      "Batch 31: 5 tickers (151–155 of 679)\n",
      "Batch 32: 5 tickers (156–160 of 679)\n",
      "Batch 33: 5 tickers (161–165 of 679)\n",
      "Batch 34: 5 tickers (166–170 of 679)\n",
      "Batch 35: 5 tickers (171–175 of 679)\n",
      "Batch 36: 5 tickers (176–180 of 679)\n",
      "Batch 37: 5 tickers (181–185 of 679)\n",
      "Batch 38: 5 tickers (186–190 of 679)\n",
      "Batch 39: 5 tickers (191–195 of 679)\n",
      "Batch 40: 5 tickers (196–200 of 679)\n",
      "Batch 41: 5 tickers (201–205 of 679)\n",
      "Batch 42: 5 tickers (206–210 of 679)\n",
      "Batch 43: 5 tickers (211–215 of 679)\n",
      "Batch 44: 5 tickers (216–220 of 679)\n",
      "Batch 45: 5 tickers (221–225 of 679)\n",
      "Batch 46: 5 tickers (226–230 of 679)\n",
      "Batch 47: 5 tickers (231–235 of 679)\n",
      "Batch 48: 5 tickers (236–240 of 679)\n",
      "Batch 49: 5 tickers (241–245 of 679)\n",
      "Batch 50: 5 tickers (246–250 of 679)\n",
      "Batch 51: 5 tickers (251–255 of 679)\n",
      "Batch 52: 5 tickers (256–260 of 679)\n",
      "Batch 53: 5 tickers (261–265 of 679)\n",
      "Batch 54: 5 tickers (266–270 of 679)\n",
      "Batch 55: 5 tickers (271–275 of 679)\n",
      "Batch 56: 5 tickers (276–280 of 679)\n",
      "Batch 57: 5 tickers (281–285 of 679)\n",
      "Batch 58: 5 tickers (286–290 of 679)\n",
      "Batch 59: 5 tickers (291–295 of 679)\n",
      "Batch 60: 5 tickers (296–300 of 679)\n",
      "Batch 61: 5 tickers (301–305 of 679)\n",
      "Batch 62: 5 tickers (306–310 of 679)\n",
      "Batch 63: 5 tickers (311–315 of 679)\n",
      "Batch 64: 5 tickers (316–320 of 679)\n",
      "Batch 65: 5 tickers (321–325 of 679)\n",
      "Batch 66: 5 tickers (326–330 of 679)\n",
      "Batch 67: 5 tickers (331–335 of 679)\n",
      "Batch 68: 5 tickers (336–340 of 679)\n",
      "Batch 69: 5 tickers (341–345 of 679)\n",
      "Batch 70: 5 tickers (346–350 of 679)\n",
      "Batch 71: 5 tickers (351–355 of 679)\n",
      "Batch 72: 5 tickers (356–360 of 679)\n",
      "Batch 73: 5 tickers (361–365 of 679)\n",
      "Batch 74: 5 tickers (366–370 of 679)\n",
      "Batch 75: 5 tickers (371–375 of 679)\n",
      "Batch 76: 5 tickers (376–380 of 679)\n",
      "Batch 77: 5 tickers (381–385 of 679)\n",
      "Batch 78: 5 tickers (386–390 of 679)\n",
      "Batch 79: 5 tickers (391–395 of 679)\n",
      "Batch 80: 5 tickers (396–400 of 679)\n",
      "Batch 81: 5 tickers (401–405 of 679)\n",
      "Batch 82: 5 tickers (406–410 of 679)\n",
      "Batch 83: 5 tickers (411–415 of 679)\n",
      "Batch 84: 5 tickers (416–420 of 679)\n",
      "Batch 85: 5 tickers (421–425 of 679)\n",
      "Batch 86: 5 tickers (426–430 of 679)\n",
      "Batch 87: 5 tickers (431–435 of 679)\n",
      "Batch 88: 5 tickers (436–440 of 679)\n",
      "Batch 89: 5 tickers (441–445 of 679)\n",
      "Batch 90: 5 tickers (446–450 of 679)\n",
      "Batch 91: 5 tickers (451–455 of 679)\n",
      "Batch 92: 5 tickers (456–460 of 679)\n",
      "Batch 93: 5 tickers (461–465 of 679)\n",
      "Batch 94: 5 tickers (466–470 of 679)\n",
      "Batch 95: 5 tickers (471–475 of 679)\n",
      "Batch 96: 5 tickers (476–480 of 679)\n",
      "Batch 97: 5 tickers (481–485 of 679)\n",
      "Batch 98: 5 tickers (486–490 of 679)\n",
      "Batch 99: 5 tickers (491–495 of 679)\n",
      "Batch 100: 5 tickers (496–500 of 679)\n",
      "Batch 101: 5 tickers (501–505 of 679)\n",
      "Batch 102: 5 tickers (506–510 of 679)\n",
      "Batch 103: 5 tickers (511–515 of 679)\n",
      "Batch 104: 5 tickers (516–520 of 679)\n",
      "Batch 105: 5 tickers (521–525 of 679)\n",
      "Batch 106: 5 tickers (526–530 of 679)\n",
      "Batch 107: 5 tickers (531–535 of 679)\n",
      "Batch 108: 5 tickers (536–540 of 679)\n",
      "Batch 109: 5 tickers (541–545 of 679)\n",
      "Batch 110: 5 tickers (546–550 of 679)\n",
      "Batch 111: 5 tickers (551–555 of 679)\n",
      "Batch 112: 5 tickers (556–560 of 679)\n",
      "Batch 113: 5 tickers (561–565 of 679)\n",
      "Batch 114: 5 tickers (566–570 of 679)\n",
      "Batch 115: 5 tickers (571–575 of 679)\n",
      "Batch 116: 5 tickers (576–580 of 679)\n",
      "Batch 117: 5 tickers (581–585 of 679)\n",
      "Batch 118: 5 tickers (586–590 of 679)\n",
      "Batch 119: 5 tickers (591–595 of 679)\n",
      "Batch 120: 5 tickers (596–600 of 679)\n",
      "Batch 121: 5 tickers (601–605 of 679)\n",
      "Batch 122: 5 tickers (606–610 of 679)\n",
      "Batch 123: 5 tickers (611–615 of 679)\n",
      "Batch 124: 5 tickers (616–620 of 679)\n",
      "Batch 125: 5 tickers (621–625 of 679)\n",
      "Batch 126: 5 tickers (626–630 of 679)\n",
      "Batch 127: 5 tickers (631–635 of 679)\n",
      "Batch 128: 5 tickers (636–640 of 679)\n",
      "Batch 129: 5 tickers (641–645 of 679)\n",
      "Batch 130: 5 tickers (646–650 of 679)\n",
      "Batch 131: 5 tickers (651–655 of 679)\n",
      "Batch 132: 5 tickers (656–660 of 679)\n",
      "Batch 133: 5 tickers (661–665 of 679)\n",
      "Batch 134: 5 tickers (666–670 of 679)\n",
      "Batch 135: 5 tickers (671–675 of 679)\n",
      "Batch 136: 4 tickers (676–679 of 679)\n",
      "✅ Success: 629 ticker chunks; rows: 16007\n",
      "  symbol       date  estimatedRevenueLow  estimatedRevenueHigh  \\\n",
      "0      A 2000-10-31         6408340381.0          9612510572.0   \n",
      "1      A 2001-10-31         9764489069.0         14646733604.0   \n",
      "2      A 2002-10-31         6569439881.0          9854159822.0   \n",
      "3      A 2003-10-31         7053692752.0         10580539129.0   \n",
      "4      A 2004-10-31         5755443808.0          8633165712.0   \n",
      "5      A 2005-10-31         2728735443.0          4093103165.0   \n",
      "6      A 2006-10-31         3800380233.0          5700570350.0   \n",
      "7      A 2007-10-31         4361631723.0          6542447586.0   \n",
      "8      A 2008-10-31         4488616211.0          6732924318.0   \n",
      "9      A 2009-10-31         3469851666.0          5204777499.0   \n",
      "\n",
      "   estimatedRevenueAvg  estimatedEbitdaLow  estimatedEbitdaHigh  \\\n",
      "0         8010425477.0        1019924930.0         1529887396.0   \n",
      "1        12205611337.0         652013727.0         1130477734.0   \n",
      "2         8211799852.0       -1171221560.0         -780814372.0   \n",
      "3         8817115941.0        -894900572.0         -485933715.0   \n",
      "4         7194304760.0         584144281.0          876216421.0   \n",
      "5         3410919304.0         512292634.0          768438952.0   \n",
      "6         4750475292.0        2673112607.0         4009668912.0   \n",
      "7         5452039655.0         692578506.0         1038867760.0   \n",
      "8         5610770265.0         788411170.0         1182616756.0   \n",
      "9         4337314583.0         152807679.0          229211517.0   \n",
      "\n",
      "   estimatedEbitdaAvg  estimatedEbitLow  estimatedEbitHigh  ...  \\\n",
      "0        1274906163.0       636910617.0        955365927.0  ...   \n",
      "1         891245731.0     -1213906039.0       -648070692.0  ...   \n",
      "2        -976017966.0     -2317525251.0      -1545016833.0  ...   \n",
      "3        -690417144.0     -1040747429.0       -656498286.0  ...   \n",
      "4         730180351.0       307855215.0        461782822.0  ...   \n",
      "5         640365793.0       104079833.0        238822453.0  ...   \n",
      "6        3341390760.0       348726667.0        523090002.0  ...   \n",
      "7         865723133.0       469401598.0        704102396.0  ...   \n",
      "8         985513963.0       616814146.0        925221220.0  ...   \n",
      "9         191009598.0         2095991.0         50780350.0  ...   \n",
      "\n",
      "   estimatedNetIncomeHigh  estimatedNetIncomeAvg  estimatedSgaExpenseLow  \\\n",
      "0             691853668.0            576544724.0            1843901385.0   \n",
      "1             252123560.0            114160111.0            2510055369.0   \n",
      "2            -989760996.0          -1237201246.0            2655394284.0   \n",
      "3           -2340564572.0          -2932205715.0            1815803427.0   \n",
      "4             416965814.0            347471512.0            1444775030.0   \n",
      "5             375483925.0            312903271.0            1231757346.0   \n",
      "6            3712610173.0           3093841811.0            1246009092.0   \n",
      "7             769331310.0            641109425.0            1368048749.0   \n",
      "8             804773443.0            670644536.0            1316896658.0   \n",
      "9              26530998.0            -15587804.0            1131045509.0   \n",
      "\n",
      "   estimatedSgaExpenseHigh  estimatedSgaExpenseAvg  estimatedEpsAvg  \\\n",
      "0             2765852079.0            2304876732.0            0.965   \n",
      "1             3765083053.0            3137569211.0            -0.26   \n",
      "2             3983091428.0            3319242856.0            -1.08   \n",
      "3             2723705141.0            2269754284.0            -0.48   \n",
      "4             2167162545.0            1805968788.0             1.05   \n",
      "5             1847636019.0            1539696683.0             0.98   \n",
      "6             1869013640.0            1557511366.0             1.56   \n",
      "7             2052073125.0            1710060937.0             1.85   \n",
      "8             1975344988.0            1646120823.0             1.96   \n",
      "9             1696568265.0            1413806887.0              0.7   \n",
      "\n",
      "   estimatedEpsHigh  estimatedEpsLow  numberAnalystEstimatedRevenue  \\\n",
      "0              1.16             0.77                           15.0   \n",
      "1              0.09            -0.61                            9.0   \n",
      "2             -0.87            -1.29                           20.0   \n",
      "3             -0.37            -0.59                            7.0   \n",
      "4              1.26             0.84                           10.0   \n",
      "5              1.18             0.78                           13.0   \n",
      "6              1.88             1.24                           12.0   \n",
      "7              2.22             1.48                            8.0   \n",
      "8              2.35             1.57                           15.0   \n",
      "9              0.84             0.56                           20.0   \n",
      "\n",
      "   numberAnalystsEstimatedEps  \n",
      "0                        15.0  \n",
      "1                         9.0  \n",
      "2                        20.0  \n",
      "3                         7.0  \n",
      "4                        10.0  \n",
      "5                        13.0  \n",
      "6                        12.0  \n",
      "7                         8.0  \n",
      "8                        15.0  \n",
      "9                        20.0  \n",
      "\n",
      "[10 rows x 22 columns]\n",
      "      symbol       date  estimatedRevenueLow  estimatedRevenueHigh  \\\n",
      "15997    ZTS 2020-12-31         6518781550.0          6690053010.0   \n",
      "15998    ZTS 2021-12-31         7647715223.0          7848647766.0   \n",
      "15999    ZTS 2022-12-31         7904003143.0          8182628022.0   \n",
      "16000    ZTS 2023-12-31         8502540678.0          8542552635.0   \n",
      "16001    ZTS 2024-12-31         9229198253.0          9259195647.0   \n",
      "16002    ZTS 2025-12-31         9458180939.0          9558267510.0   \n",
      "16003    ZTS 2026-12-31         9933127631.0         10232920216.0   \n",
      "16004    ZTS 2027-12-31        10650875527.0         10661197361.0   \n",
      "16005    ZTS 2028-12-31        11050265071.0         11275901608.0   \n",
      "16006    ZTS 2029-12-31        11662684825.0         11900826427.0   \n",
      "\n",
      "       estimatedRevenueAvg  estimatedEbitdaLow  estimatedEbitdaHigh  \\\n",
      "15997         6600084363.0        1380711526.0         2071067290.0   \n",
      "15998         7743098195.0        2199566701.0         3299350053.0   \n",
      "15999         8042479843.0        2493129992.0         3739694989.0   \n",
      "16000         8522546657.0        2742442990.0         4113664487.0   \n",
      "16001         9249196516.0        3831100996.0         3843553112.0   \n",
      "16002         9518232882.0        3926153218.0         3967699813.0   \n",
      "16003        10083023924.0        4123306719.0         4247752596.0   \n",
      "16004        10656036444.0        4421248599.0         4425533260.0   \n",
      "16005        11167104837.0        4587037830.0         4680701043.0   \n",
      "16006        11786000000.0        4841257304.0         4940111452.0   \n",
      "\n",
      "       estimatedEbitdaAvg  estimatedEbitLow  estimatedEbitHigh  ...  \\\n",
      "15997        1725889408.0      1474462517.0       2211693775.0  ...   \n",
      "15998        2749458377.0      1949248770.0       2923873157.0  ...   \n",
      "15999        3116412491.0      2236239730.0       3354359595.0  ...   \n",
      "16000        3428053739.0      2459863700.0       3689795553.0  ...   \n",
      "16001        3839402407.0      3291391846.0       3302089761.0  ...   \n",
      "16002        3951081175.0      3373053516.0       3408747205.0  ...   \n",
      "16003        4185529658.0      3542432874.0       3649347347.0  ...   \n",
      "16004        4423390930.0      3798401974.0       3802083031.0  ...   \n",
      "16005        4635538786.0      3940835526.0       4021303866.0  ...   \n",
      "16006        4892446246.0      4159241647.0       4244169644.0  ...   \n",
      "\n",
      "       estimatedNetIncomeHigh  estimatedNetIncomeAvg  estimatedSgaExpenseLow  \\\n",
      "15997            1601202896.0           1334335747.0            1139928403.0   \n",
      "15998            2121004656.0           1767503880.0            1412654102.0   \n",
      "15999            2415469850.0           2012891541.0            1615878193.0   \n",
      "16000            2812800001.0           2344000000.0            1777466010.0   \n",
      "16001            2696229786.0           2680341945.5            2338186754.0   \n",
      "16002            2907552167.0           2875651402.5            2396198758.0   \n",
      "16003            3204413264.0           3142613066.0            2516524926.0   \n",
      "16004            3652793326.0           3420254559.5            2698363974.0   \n",
      "16005            3853576879.0           3805381185.0            2799547990.0   \n",
      "16006            4122451178.0           4070889608.0            2954702503.0   \n",
      "\n",
      "       estimatedSgaExpenseHigh  estimatedSgaExpenseAvg  estimatedEpsAvg  \\\n",
      "15997             1709892604.0            1424910504.0          3.79619   \n",
      "15998             2118981153.0            1765817628.0          4.66786   \n",
      "15999             2423817292.0            2019847743.0           4.8797   \n",
      "16000             2666199018.0            2221832515.0          5.41105   \n",
      "16001             2345786494.0            2343253248.0          5.88784   \n",
      "16002             2421555359.0            2411412718.0          6.35228   \n",
      "16003             2592476383.0            2554500655.0          6.90412   \n",
      "16004             2700978977.0            2699671476.0          7.55094   \n",
      "16005             2856712258.0            2829148957.0          8.36627   \n",
      "16006             3015034888.0            2985943993.0             8.95   \n",
      "\n",
      "       estimatedEpsHigh  estimatedEpsLow  numberAnalystEstimatedRevenue  \\\n",
      "15997           3.86346           3.7354                            5.0   \n",
      "15998           4.75058          4.59311                            6.0   \n",
      "15999           4.99024          4.77047                            6.0   \n",
      "16000           5.43105          5.39105                            9.0   \n",
      "16001           5.92776           5.8579                           12.0   \n",
      "16002           6.39236          6.25209                           13.0   \n",
      "16003           7.04502          6.77328                           13.0   \n",
      "16004            8.0308          7.00831                           14.0   \n",
      "16005           8.47223          8.25247                            6.0   \n",
      "16006           9.06336          8.82826                            9.0   \n",
      "\n",
      "       numberAnalystsEstimatedEps  \n",
      "15997                         4.0  \n",
      "15998                         5.0  \n",
      "15999                         7.0  \n",
      "16000                        12.0  \n",
      "16001                        15.0  \n",
      "16002                        12.0  \n",
      "16003                        13.0  \n",
      "16004                         8.0  \n",
      "16005                         7.0  \n",
      "16006                         6.0  \n",
      "\n",
      "[10 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# ===================== FMP Analyst Estimates — Full History + Postgres Upsert =====================\n",
    "import time\n",
    "from typing import Iterable, List, Optional, Tuple, Union, Sequence, Set, Dict\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------- Config --------------------------\n",
    "FMP_AE_BASE = \"https://financialmodelingprep.com/api/v3/analyst-estimates\"\n",
    "\n",
    "# --- Postgres (optional; used by upsert_analyst_estimates_postgres at the bottom) ---\n",
    "PG_CONN_STR = \"postgresql://postgres:CSDBMS623@localhost:5432/SP500_ML\"\n",
    "SCHEMA      = \"public\"\n",
    "TABLE       = \"analyst_estimates_q\"      # change if you prefer another table name\n",
    "CHUNK_ROWS  = 25_000\n",
    "\n",
    "# -------------------------- HTTP helpers --------------------------\n",
    "def _get_with_retries(\n",
    "    session: requests.Session,\n",
    "    url: str,\n",
    "    params: dict,\n",
    "    timeout: int = 30,\n",
    "    max_retries: int = 4,\n",
    "    base_sleep: float = 1.0,\n",
    "):\n",
    "    \"\"\"GET with simple exponential backoff on transient errors/rate-limits.\"\"\"\n",
    "    last = None\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        resp = session.get(url, params=params, timeout=timeout)\n",
    "        if resp.status_code == 200:\n",
    "            return resp\n",
    "        last = resp\n",
    "        if resp.status_code in (429, 500, 502, 503, 504):\n",
    "            time.sleep(base_sleep * (2 ** (attempt - 1)))\n",
    "            continue\n",
    "        resp.raise_for_status()\n",
    "    if last is not None:\n",
    "        last.raise_for_status()\n",
    "    raise RuntimeError(\"Request failed without response.\")\n",
    "\n",
    "def _normalize_fmp_json(j):\n",
    "    \"\"\"Normalize possible JSON shapes to list[dict].\"\"\"\n",
    "    if isinstance(j, list):\n",
    "        return j\n",
    "    if isinstance(j, dict):\n",
    "        # surface API messages as exceptions\n",
    "        for k in (\"error\", \"Error\", \"message\", \"Note\", \"Error Message\"):\n",
    "            if k in j and isinstance(j[k], str):\n",
    "                raise RuntimeError(f\"API message: {j[k]}\")\n",
    "        # common list-bearing keys\n",
    "        for k in (\"items\", \"data\", \"results\", \"analystEstimates\", \"historical\"):\n",
    "            if k in j and isinstance(j[k], list):\n",
    "                return j[k]\n",
    "        # fallback: single record\n",
    "        return [j]\n",
    "    raise RuntimeError(f\"Unexpected JSON type from API: {type(j)}\")\n",
    "\n",
    "# -------------------------- Core fetchers --------------------------\n",
    "def fetch_analyst_estimates_one(\n",
    "    ticker: str,\n",
    "    api_key: str,\n",
    "    session: Optional[requests.Session] = None,\n",
    "    timeout: int = 30,\n",
    "    per_call_limit: int = 50000,\n",
    "    start: Optional[Union[str, pd.Timestamp]] = None,   # None => no lower bound\n",
    "    end:   Optional[Union[str, pd.Timestamp]] = None,   # None => no upper bound\n",
    "    include_future: bool = True,                         # keep forward dates by default\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch *all available* analyst estimates for a single ticker.\n",
    "    - Does NOT cap by 'today' unless include_future=False or end is provided.\n",
    "    - Returns a tidy DataFrame sorted by symbol/date, with numeric fields coerced.\n",
    "    \"\"\"\n",
    "    if session is None:\n",
    "        session = requests.Session()\n",
    "\n",
    "    url = f\"{FMP_AE_BASE}/{ticker.upper()}\"\n",
    "    params = {\"apikey\": api_key, \"limit\": per_call_limit}\n",
    "\n",
    "    r = _get_with_retries(session, url, params, timeout=timeout)\n",
    "    try:\n",
    "        data = r.json()\n",
    "    except ValueError as e:\n",
    "        raise RuntimeError(f\"Non-JSON response for {ticker}: {r.text[:300]}\") from e\n",
    "\n",
    "    rows = _normalize_fmp_json(data)\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=[\"symbol\", \"date\"])\n",
    "\n",
    "    df = pd.DataFrame.from_records(rows)\n",
    "\n",
    "    # Robust symbol/date handling\n",
    "    if \"symbol\" in df.columns:\n",
    "        df[\"symbol\"] = df[\"symbol\"].astype(str)\n",
    "        miss = df[\"symbol\"].isin([\"\", \"None\", \"nan\", \"NaN\"]) | df[\"symbol\"].isna()\n",
    "        df.loc[miss, \"symbol\"] = ticker\n",
    "        df[\"symbol\"] = df[\"symbol\"].str.upper()\n",
    "    else:\n",
    "        df[\"symbol\"] = ticker.upper()\n",
    "\n",
    "    if \"date\" not in df.columns:\n",
    "        raise RuntimeError(f\"'date' missing in analyst-estimates payload for {ticker}.\")\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "\n",
    "    # Coerce numerics where possible (leave text cols as-is)\n",
    "    for c in df.columns:\n",
    "        if c in (\"symbol\", \"date\"):\n",
    "            continue\n",
    "        # try numeric; keep NAs\n",
    "        ser = pd.to_numeric(df[c], errors=\"ignore\")\n",
    "        if pd.api.types.is_float_dtype(ser) or pd.api.types.is_integer_dtype(ser):\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"Float64\")\n",
    "        else:\n",
    "            df[c] = ser\n",
    "\n",
    "    # Drop bad dates, de-dup, sort\n",
    "    df = (\n",
    "        df.dropna(subset=[\"date\"])\n",
    "          .drop_duplicates(subset=[\"symbol\", \"date\"], keep=\"last\")\n",
    "          .sort_values([\"symbol\", \"date\"])\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Date filtering\n",
    "    if start is not None:\n",
    "        df = df[df[\"date\"] >= pd.to_datetime(start)]\n",
    "    if end is not None:\n",
    "        df = df[df[\"date\"] <= pd.to_datetime(end)]\n",
    "    elif not include_future:\n",
    "        df = df[df[\"date\"] <= pd.Timestamp.today().normalize()]\n",
    "\n",
    "    # Keep base + all provided estimate fields\n",
    "    base = [\"symbol\", \"date\"]\n",
    "    extras = [c for c in df.columns if c not in base]\n",
    "    df = df[base + extras]\n",
    "    return df\n",
    "\n",
    "\n",
    "def fetch_analyst_estimates(\n",
    "    tickers: Union[str, Iterable[str]],\n",
    "    api_key: str,\n",
    "    batch_size: int = 25,\n",
    "    sleep_between_batches: float = 1.0,\n",
    "    timeout: int = 30,\n",
    "    skip_errors: bool = True,\n",
    "    verbose: bool = True,\n",
    "    start: Optional[Union[str, pd.Timestamp]] = None,  # None => no lower bound\n",
    "    end:   Optional[Union[str, pd.Timestamp]] = None,  # None => no upper bound\n",
    "    include_future: bool = True,                        # keep forward dates\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Batched pull across many tickers; returns concatenated tidy DataFrame.\"\"\"\n",
    "    if isinstance(tickers, str):\n",
    "        tickers = [tickers]\n",
    "    tickers = [t.upper().strip() for t in tickers if str(t).strip()]\n",
    "\n",
    "    session = requests.Session()\n",
    "    frames: List[pd.DataFrame] = []\n",
    "    skipped: List[Tuple[str, str]] = []\n",
    "    total = len(tickers)\n",
    "\n",
    "    for i in range(0, total, batch_size):\n",
    "        batch = tickers[i:i + batch_size]\n",
    "        if verbose:\n",
    "            print(f\"Batch {i//batch_size + 1}: {len(batch)} tickers ({i+1}–{min(i+len(batch), total)} of {total})\")\n",
    "\n",
    "        for t in batch:\n",
    "            try:\n",
    "                df_t = fetch_analyst_estimates_one(\n",
    "                    t,\n",
    "                    api_key=api_key,\n",
    "                    session=session,\n",
    "                    timeout=timeout,\n",
    "                    start=start,\n",
    "                    end=end,\n",
    "                    include_future=include_future,\n",
    "                )\n",
    "                if not df_t.empty:\n",
    "                    frames.append(df_t)\n",
    "            except Exception as e:\n",
    "                if skip_errors:\n",
    "                    skipped.append((t, str(e)))\n",
    "                    if verbose:\n",
    "                        print(f\"  [skip] {t}: {e}\")\n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "        if i + batch_size < total:\n",
    "            time.sleep(sleep_between_batches)\n",
    "\n",
    "    if not frames:\n",
    "        if verbose:\n",
    "            print(\"No successful analyst-estimates pulls.\")\n",
    "            if skipped:\n",
    "                print(f\"Skipped {len(skipped)} tickers. Examples: {skipped[:5]}\")\n",
    "        return pd.DataFrame(columns=[\"symbol\", \"date\"])\n",
    "\n",
    "    df_all = pd.concat(frames, ignore_index=True)\n",
    "    if verbose:\n",
    "        print(f\"✅ Success: {len(frames)} ticker chunks; rows: {len(df_all)}\")\n",
    "        if skipped:\n",
    "            print(f\"⚠️ Skipped {len(skipped)} tickers.\")\n",
    "    return df_all\n",
    "\n",
    "\n",
    "# -------------------------- Optional: Postgres Upsert --------------------------\n",
    "import math\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy.types import BigInteger, Float, Text, DateTime\n",
    "\n",
    "def _pg_engine(conn_str: str) -> Engine:\n",
    "    return create_engine(conn_str, pool_pre_ping=True)\n",
    "\n",
    "def _existing_columns(engine: Engine, schema: str, table: str) -> Set[str]:\n",
    "    sql = \"\"\"\n",
    "    SELECT lower(column_name) FROM information_schema.columns\n",
    "    WHERE table_schema = :schema AND table_name = :table\n",
    "    \"\"\"\n",
    "    with engine.begin() as conn:\n",
    "        rows = conn.execute(text(sql), {\"schema\": schema, \"table\": table}).fetchall()\n",
    "    return {r[0] for r in rows}\n",
    "\n",
    "def _infer_sql_type_from_series(s: pd.Series) -> str:\n",
    "    if pd.api.types.is_datetime64_any_dtype(s): return \"TIMESTAMP\"\n",
    "    if pd.api.types.is_integer_dtype(s):        return \"BIGINT\"\n",
    "    if pd.api.types.is_float_dtype(s):          return \"DOUBLE PRECISION\"\n",
    "    return \"TEXT\"\n",
    "\n",
    "def ensure_table_and_indexes(engine: Engine, schema: str, table: str):\n",
    "    ddl = f'''\n",
    "    CREATE TABLE IF NOT EXISTS \"{schema}\".\"{table}\" (\n",
    "      symbol TEXT,\n",
    "      date   TIMESTAMP\n",
    "    );'''\n",
    "    uq  = f\"\"\"\n",
    "    DO $$\n",
    "    BEGIN\n",
    "      IF NOT EXISTS (\n",
    "        SELECT 1 FROM pg_constraint WHERE conname = '{table}_symbol_date_key'\n",
    "      ) THEN\n",
    "        ALTER TABLE \"{schema}\".\"{table}\"\n",
    "        ADD CONSTRAINT {table}_symbol_date_key UNIQUE (symbol, date);\n",
    "      END IF;\n",
    "    END$$;\"\"\"\n",
    "    idx1 = f'CREATE INDEX IF NOT EXISTS {table}_symbol_idx ON \"{schema}\".\"{table}\" (symbol);'\n",
    "    idx2 = f'CREATE INDEX IF NOT EXISTS {table}_date_idx   ON \"{schema}\".\"{table}\" (date);'\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(ddl)); conn.execute(text(uq)); conn.execute(text(idx1)); conn.execute(text(idx2))\n",
    "\n",
    "def ensure_missing_columns(engine: Engine, schema: str, table: str, df: pd.DataFrame):\n",
    "    have = _existing_columns(engine, schema, table)\n",
    "    missing = [c for c in df.columns if c not in have]\n",
    "    if not missing:\n",
    "        return\n",
    "    alters = []\n",
    "    for c in missing:\n",
    "        sql_t = _infer_sql_type_from_series(df[c])\n",
    "        alters.append(f'ADD COLUMN IF NOT EXISTS {c} {sql_t}')\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(f'ALTER TABLE \"{schema}\".\"{table}\" ' + \", \".join(alters) + \";\"))\n",
    "\n",
    "def _dtype_map_for_to_sql(df: pd.DataFrame) -> Dict[str, object]:\n",
    "    dmap: Dict[str, object] = {}\n",
    "    for c in df.columns:\n",
    "        s = df[c]\n",
    "        if pd.api.types.is_datetime64_any_dtype(s):\n",
    "            dmap[c] = DateTime(timezone=False)\n",
    "        elif pd.api.types.is_integer_dtype(s):\n",
    "            dmap[c] = BigInteger()\n",
    "        elif pd.api.types.is_float_dtype(s):\n",
    "            dmap[c] = Float()\n",
    "        else:\n",
    "            dmap[c] = Text()\n",
    "    return dmap\n",
    "\n",
    "def _stage_to_sql(engine: Engine, df: pd.DataFrame, schema: str, staging: str):\n",
    "    df.to_sql(\n",
    "        name=staging, con=engine, schema=schema,\n",
    "        if_exists=\"replace\", index=False,\n",
    "        dtype=_dtype_map_for_to_sql(df),\n",
    "        chunksize=10_000,\n",
    "    )\n",
    "\n",
    "def _merge_from_staging(engine: Engine, schema: str, table: str, staging: str, cols: Sequence[str]):\n",
    "    non_key = [c for c in cols if c not in (\"symbol\", \"date\")]\n",
    "    set_clause = \", \".join([f\"{c}=EXCLUDED.{c}\" for c in non_key]) or \"symbol=EXCLUDED.symbol\"\n",
    "    sql = f\"\"\"\n",
    "    INSERT INTO \"{schema}\".\"{table}\" ({\", \".join(cols)})\n",
    "    SELECT {\", \".join(cols)} FROM \"{schema}\".\"{staging}\"\n",
    "    ON CONFLICT (symbol, date)\n",
    "    DO UPDATE SET {set_clause};\n",
    "    \"\"\"\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(sql))\n",
    "\n",
    "def upsert_analyst_estimates_postgres(\n",
    "    df: pd.DataFrame,\n",
    "    conn_str: str = PG_CONN_STR,\n",
    "    schema: str = SCHEMA,\n",
    "    table: str = TABLE,\n",
    "    chunk_rows: int = CHUNK_ROWS,\n",
    "):\n",
    "    \"\"\"\n",
    "    Upserts the analyst estimates DataFrame to Postgres:\n",
    "      - lowercases column names\n",
    "      - ensures table + unique(symbol,date) + indexes\n",
    "      - auto-adds any extra columns present in df\n",
    "      - chunked staging + merge\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"DataFrame is empty; nothing to ingest.\")\n",
    "        return\n",
    "\n",
    "    d = df.copy()\n",
    "    d.columns = d.columns.str.lower()\n",
    "    d[\"symbol\"] = d[\"symbol\"].astype(str).str.upper()\n",
    "    d[\"date\"]   = pd.to_datetime(d[\"date\"], errors=\"coerce\").dt.tz_localize(None)\n",
    "\n",
    "    eng = _pg_engine(conn_str)\n",
    "    ensure_table_and_indexes(eng, schema, table)\n",
    "    ensure_missing_columns(eng, schema, table, d)\n",
    "\n",
    "    # keys first\n",
    "    key_first = [c for c in (\"symbol\",\"date\") if c in d.columns]\n",
    "    rest = [c for c in d.columns if c not in key_first]\n",
    "    d = d[key_first + rest]\n",
    "\n",
    "    # chunked upsert\n",
    "    n = len(d); n_chunks = math.ceil(n / chunk_rows)\n",
    "    for i in range(n_chunks):\n",
    "        lo, hi = i*chunk_rows, min((i+1)*chunk_rows, n)\n",
    "        staging = f\"stg_{table}\"\n",
    "        _stage_to_sql(eng, d.iloc[lo:hi].copy(), schema, staging)\n",
    "        _merge_from_staging(eng, schema, table, staging, d.columns.tolist())\n",
    "        print(f\"Upserted rows {lo}–{hi} / {n}\")\n",
    "    print(\"✅ Analyst estimates ingestion complete.\")\n",
    "\n",
    "\n",
    "# -------------------------- Example usage --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    API_KEY = \"\"\n",
    "    # e.g., your big list (same as you use elsewhere)\n",
    "    tickers = universe\n",
    "\n",
    "    # Pull *all* available data (no start/end caps, include future periods)\n",
    "    df_ae = fetch_analyst_estimates(\n",
    "        tickers=tickers,\n",
    "        api_key=API_KEY,\n",
    "        start=None,\n",
    "        end=None,\n",
    "        include_future=True,\n",
    "        batch_size=5,\n",
    "        sleep_between_batches=3.0,\n",
    "        skip_errors=True,\n",
    "        verbose=True,\n",
    "    )\n",
    "    print(df_ae.head(10))\n",
    "    print(df_ae.tail(10))\n",
    "\n",
    "    # Ingest to Postgres (optional)\n",
    "    # upsert_analyst_estimates_postgres(df_ae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78c08f1-b2c9-4732-9fcb-95172d926b86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
